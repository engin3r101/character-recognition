{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUIuon20T8uB4IYRdk8ft7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engin3r101/character-recognition/blob/main/Character_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIzkFE_aE1sw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json here\n"
      ],
      "metadata": {
        "id": "smtC_NcuF96F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "cSVKF3VXGnjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d vaibhao/handwritten-characters"
      ],
      "metadata": {
        "id": "fxEr-uK0He6x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip handwritten-characters.zip -d ./data"
      ],
      "metadata": {
        "id": "OrtMTkp2HnR7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = './data/Train'\n",
        "\n",
        "image_size = (32, 32) # Define your desired image size\n",
        "batch_size = 32 # Define your desired batch size\n",
        "\n",
        "image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    labels='inferred', # Infer labels from subdirectory names\n",
        "    label_mode='int',   # Labels as integers\n",
        "    image_size=image_size,\n",
        "    interpolation='nearest',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgRugaICH0En",
        "outputId": "4d2d5020-ce82-446b-a3d2-811cfcaad9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 834036 files belonging to 39 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dataset.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J9XBl_eCKT_y",
        "outputId": "00fdb567-3e87-452c-ca33-4cab7ce913d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " '$',\n",
              " '&',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(image_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YTLP-fvkMkUQ",
        "outputId": "698428fe-2558-416e-ffa0-b63b834f3006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on _PrefetchDataset in module tensorflow.python.data.ops.prefetch_op object:\n",
            "\n",
            "class _PrefetchDataset(tensorflow.python.data.ops.dataset_ops.UnaryUnchangedStructureDataset)\n",
            " |  _PrefetchDataset(input_dataset, buffer_size, slack_period=None, name=None)\n",
            " |\n",
            " |  A `Dataset` that asynchronously prefetches its input.\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      _PrefetchDataset\n",
            " |      tensorflow.python.data.ops.dataset_ops.UnaryUnchangedStructureDataset\n",
            " |      tensorflow.python.data.ops.dataset_ops.UnaryDataset\n",
            " |      tensorflow.python.data.ops.dataset_ops.DatasetV2\n",
            " |      collections.abc.Iterable\n",
            " |      tensorflow.python.trackable.base.Trackable\n",
            " |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
            " |      tensorflow.python.types.data.DatasetV2\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, input_dataset, buffer_size, slack_period=None, name=None)\n",
            " |      See `Dataset.prefetch()` for details.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from tensorflow.python.data.ops.dataset_ops.UnaryUnchangedStructureDataset:\n",
            " |\n",
            " |  element_spec\n",
            " |      The type specification of an element of this dataset.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> dataset.element_spec\n",
            " |      TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
            " |\n",
            " |      For more information,\n",
            " |      read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).\n",
            " |\n",
            " |      Returns:\n",
            " |        A (nested) structure of `tf.TypeSpec` objects matching the structure of an\n",
            " |        element of this dataset and specifying the type of individual components.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.data.ops.dataset_ops.DatasetV2:\n",
            " |\n",
            " |  __bool__(self)\n",
            " |\n",
            " |  __debug_string__(self)\n",
            " |      Returns a string showing the type of the dataset and its inputs.\n",
            " |\n",
            " |      This string is intended only for debugging purposes, and may change without\n",
            " |      warning.\n",
            " |\n",
            " |  __iter__(self) -> tensorflow.python.data.ops.iterator_ops.OwnedIterator\n",
            " |      Creates an iterator for elements of this dataset.\n",
            " |\n",
            " |      The returned iterator implements the Python Iterator protocol.\n",
            " |\n",
            " |      Returns:\n",
            " |        An `tf.data.Iterator` for the elements of this dataset.\n",
            " |\n",
            " |      Raises:\n",
            " |        RuntimeError: If not inside of tf.function and not executing eagerly.\n",
            " |\n",
            " |  __len__(self)\n",
            " |      Returns the length of the dataset if it is known and finite.\n",
            " |\n",
            " |      This method requires that you are running in eager mode, and that the\n",
            " |      length of the dataset is known and non-infinite. When the length may be\n",
            " |      unknown or infinite, or if you are running in graph mode, use\n",
            " |      `tf.data.Dataset.cardinality` instead.\n",
            " |\n",
            " |      Returns:\n",
            " |        An integer representing the length of the dataset.\n",
            " |\n",
            " |      Raises:\n",
            " |        RuntimeError: If the dataset length is unknown or infinite, or if eager\n",
            " |          execution is not enabled.\n",
            " |\n",
            " |  __nonzero__ = __bool__(self)\n",
            " |\n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  apply(self, transformation_func) -> 'DatasetV2'\n",
            " |      Applies a transformation function to this dataset.\n",
            " |\n",
            " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
            " |      represented as functions that take one `Dataset` argument and return a\n",
            " |      transformed `Dataset`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(100)\n",
            " |      >>> def dataset_fn(ds):\n",
            " |      ...   return ds.filter(lambda x: x < 5)\n",
            " |      >>> dataset = dataset.apply(dataset_fn)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |\n",
            " |      Args:\n",
            " |        transformation_func: A function that takes one `Dataset` argument and\n",
            " |          returns a `Dataset`.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  as_numpy_iterator(self)\n",
            " |      Returns an iterator which converts all elements of the dataset to numpy.\n",
            " |\n",
            " |      Use `as_numpy_iterator` to inspect the content of your dataset. To see\n",
            " |      element shapes and types, print dataset elements directly instead of using\n",
            " |      `as_numpy_iterator`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> for element in dataset:\n",
            " |      ...   print(element)\n",
            " |      tf.Tensor(1, shape=(), dtype=int32)\n",
            " |      tf.Tensor(2, shape=(), dtype=int32)\n",
            " |      tf.Tensor(3, shape=(), dtype=int32)\n",
            " |\n",
            " |      This method requires that you are running in eager mode and the dataset's\n",
            " |      element_spec contains only `TensorSpec` components.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> for element in dataset.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      1\n",
            " |      2\n",
            " |      3\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2, 3]\n",
            " |\n",
            " |      `as_numpy_iterator()` will preserve the nested structure of dataset\n",
            " |      elements.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
            " |      ...                                               'b': [5, 6]})\n",
            " |      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
            " |      ...                                       {'a': (2, 4), 'b': 6}]\n",
            " |      True\n",
            " |\n",
            " |      Returns:\n",
            " |        An iterable over the elements of the dataset, with their tensors converted\n",
            " |        to numpy arrays.\n",
            " |\n",
            " |      Raises:\n",
            " |        TypeError: if an element contains a non-`Tensor` value.\n",
            " |        RuntimeError: if eager execution is not enabled.\n",
            " |\n",
            " |  batch(self, batch_size, drop_remainder=False, num_parallel_calls=None, deterministic=None, name=None) -> 'DatasetV2'\n",
            " |      Combines consecutive elements of this dataset into batches.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(8)\n",
            " |      >>> dataset = dataset.batch(3)\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(8)\n",
            " |      >>> dataset = dataset.batch(3, drop_remainder=True)\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
            " |\n",
            " |      The components of the resulting element will have an additional outer\n",
            " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
            " |      element if `batch_size` does not divide the number of input elements `N`\n",
            " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
            " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
            " |      argument to `True` to prevent the smaller batch from being produced.\n",
            " |\n",
            " |      Note: If your program requires data to have a statically known shape (e.g.,\n",
            " |      when using XLA), you should use `drop_remainder=True`. Without\n",
            " |      `drop_remainder=True` the shape of the output dataset will have an unknown\n",
            " |      leading dimension due to the possibility of a smaller final batch.\n",
            " |\n",
            " |      Args:\n",
            " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          consecutive elements of this dataset to combine in a single batch.\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last batch should be dropped in the case it has fewer than\n",
            " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
            " |          batch.\n",
            " |        num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
            " |          representing the number of batches to compute asynchronously in\n",
            " |          parallel.\n",
            " |          If not specified, batches will be computed sequentially. If the value\n",
            " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
            " |          calls is set dynamically based on available resources.\n",
            " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
            " |          boolean is specified (`True` or `False`), it controls the order in which\n",
            " |          the transformation produces elements. If set to `False`, the\n",
            " |          transformation is allowed to yield elements out of order to trade\n",
            " |          determinism for performance. If not specified, the\n",
            " |          `tf.data.Options.deterministic` option (`True` by default) controls the\n",
            " |          behavior.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  bucket_by_sequence_length(self, element_length_func, bucket_boundaries, bucket_batch_sizes, padded_shapes=None, padding_values=None, pad_to_bucket_boundary=False, no_padding=False, drop_remainder=False, name=None) -> 'DatasetV2'\n",
            " |      A transformation that buckets elements in a `Dataset` by length.\n",
            " |\n",
            " |      Elements of the `Dataset` are grouped together by length and then are padded\n",
            " |      and batched.\n",
            " |\n",
            " |      This is useful for sequence tasks in which the elements have variable\n",
            " |      length. Grouping together elements that have similar lengths reduces the\n",
            " |      total fraction of padding in a batch which increases training step\n",
            " |      efficiency.\n",
            " |\n",
            " |      Below is an example to bucketize the input data to the 3 buckets\n",
            " |      \"[0, 3), [3, 5), [5, inf)\" based on sequence length, with batch size 2.\n",
            " |\n",
            " |      >>> elements = [\n",
            " |      ...   [0], [1, 2, 3, 4], [5, 6, 7],\n",
            " |      ...   [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(\n",
            " |      ...     lambda: elements, tf.int64, output_shapes=[None])\n",
            " |      >>> dataset = dataset.bucket_by_sequence_length(\n",
            " |      ...         element_length_func=lambda elem: tf.shape(elem)[0],\n",
            " |      ...         bucket_boundaries=[3, 5],\n",
            " |      ...         bucket_batch_sizes=[2, 2, 2])\n",
            " |      >>> for elem in dataset.as_numpy_iterator():\n",
            " |      ...   print(elem)\n",
            " |      [[1 2 3 4]\n",
            " |      [5 6 7 0]]\n",
            " |      [[ 7  8  9 10 11  0]\n",
            " |      [13 14 15 16 19 20]]\n",
            " |      [[ 0  0]\n",
            " |      [21 22]]\n",
            " |\n",
            " |      Args:\n",
            " |        element_length_func: function from element in `Dataset` to `tf.int32`,\n",
            " |          determines the length of the element, which will determine the bucket it\n",
            " |          goes into.\n",
            " |        bucket_boundaries: `list<int>`, upper length boundaries of the buckets.\n",
            " |        bucket_batch_sizes: `list<int>`, batch size per bucket. Length should be\n",
            " |          `len(bucket_boundaries) + 1`.\n",
            " |        padded_shapes: Nested structure of `tf.TensorShape` to pass to\n",
            " |          `tf.data.Dataset.padded_batch`. If not provided, will use\n",
            " |          `dataset.output_shapes`, which will result in variable length dimensions\n",
            " |          being padded out to the maximum length in each batch.\n",
            " |        padding_values: Values to pad with, passed to\n",
            " |          `tf.data.Dataset.padded_batch`. Defaults to padding with 0.\n",
            " |        pad_to_bucket_boundary: bool, if `False`, will pad dimensions with unknown\n",
            " |          size to maximum length in batch. If `True`, will pad dimensions with\n",
            " |          unknown size to bucket boundary minus 1 (i.e., the maximum length in\n",
            " |          each bucket), and caller must ensure that the source `Dataset` does not\n",
            " |          contain any elements with length longer than `max(bucket_boundaries)`.\n",
            " |        no_padding: `bool`, indicates whether to pad the batch features (features\n",
            " |          need to be either of type `tf.sparse.SparseTensor` or of same shape).\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last batch should be dropped in the case it has fewer than\n",
            " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
            " |          batch.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError: if `len(bucket_batch_sizes) != len(bucket_boundaries) + 1`.\n",
            " |\n",
            " |  cache(self, filename='', name=None) -> 'DatasetV2'\n",
            " |      Caches the elements in this dataset.\n",
            " |\n",
            " |      The first time the dataset is iterated over, its elements will be cached\n",
            " |      either in the specified file or in memory. Subsequent iterations will\n",
            " |      use the cached data.\n",
            " |\n",
            " |      Note: To guarantee that the cache gets finalized, the input dataset must be\n",
            " |      iterated through in its entirety, until it raises StopIteration. Otherwise,\n",
            " |      subsequent iterations may not use cached data.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(5)\n",
            " |      >>> dataset = dataset.map(lambda x: x**2)\n",
            " |      >>> dataset = dataset.cache()\n",
            " |      >>> # The first time reading through the data will generate the data using\n",
            " |      >>> # `range` and `map`.\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 4, 9, 16]\n",
            " |      >>> # Subsequent iterations read from the cache.\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 4, 9, 16]\n",
            " |\n",
            " |      When caching to a file, the cached data will persist across runs. Even the\n",
            " |      first iteration through the data will read from the cache file. Changing\n",
            " |      the input pipeline before the call to `.cache()` will have no effect until\n",
            " |      the cache file is removed or the filename is changed.\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = tf.data.Dataset.range(5)\n",
            " |      dataset = dataset.cache(\"/path/to/file\")\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [0, 1, 2, 3, 4]\n",
            " |      dataset = tf.data.Dataset.range(10)\n",
            " |      dataset = dataset.cache(\"/path/to/file\")  # Same file!\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [0, 1, 2, 3, 4]\n",
            " |      ```\n",
            " |\n",
            " |      Note: `cache` will produce exactly the same elements during each iteration\n",
            " |      through the dataset. If you wish to randomize the iteration order, make sure\n",
            " |      to call `shuffle` *after* calling `cache`.\n",
            " |\n",
            " |      Args:\n",
            " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
            " |          directory on the filesystem to use for caching elements in this Dataset.\n",
            " |          If a filename is not provided, the dataset will be cached in memory.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  cardinality(self)\n",
            " |      Returns the cardinality of the dataset, if known.\n",
            " |\n",
            " |      `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\n",
            " |      contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\n",
            " |      the analysis fails to determine the number of elements in the dataset\n",
            " |      (e.g. when the dataset source is a file).\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(42)\n",
            " |      >>> print(dataset.cardinality().numpy())\n",
            " |      42\n",
            " |      >>> dataset = dataset.repeat()\n",
            " |      >>> cardinality = dataset.cardinality()\n",
            " |      >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
            " |      True\n",
            " |      >>> dataset = dataset.filter(lambda x: True)\n",
            " |      >>> cardinality = dataset.cardinality()\n",
            " |      >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n",
            " |      True\n",
            " |\n",
            " |      Returns:\n",
            " |        A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\n",
            " |        If the cardinality is infinite or unknown, `cardinality` returns the\n",
            " |        named constants `tf.data.INFINITE_CARDINALITY` and\n",
            " |        `tf.data.UNKNOWN_CARDINALITY` respectively.\n",
            " |\n",
            " |  concatenate(self, dataset, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
            " |\n",
            " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
            " |      >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
            " |      >>> ds = a.concatenate(b)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [1, 2, 3, 4, 5, 6, 7]\n",
            " |      >>> # The input dataset and dataset to be concatenated should have\n",
            " |      >>> # compatible element specs.\n",
            " |      >>> c = tf.data.Dataset.zip((a, b))\n",
            " |      >>> a.concatenate(c)\n",
            " |      Traceback (most recent call last):\n",
            " |      TypeError: Two datasets to concatenate have different types\n",
            " |      <dtype: 'int64'> and (tf.int64, tf.int64)\n",
            " |      >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
            " |      >>> a.concatenate(d)\n",
            " |      Traceback (most recent call last):\n",
            " |      TypeError: Two datasets to concatenate have different types\n",
            " |      <dtype: 'int64'> and <dtype: 'string'>\n",
            " |\n",
            " |      Args:\n",
            " |        dataset: `Dataset` to be concatenated.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  enumerate(self, start=0, name=None) -> 'DatasetV2'\n",
            " |      Enumerates the elements of this dataset.\n",
            " |\n",
            " |      It is similar to python's `enumerate`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> dataset = dataset.enumerate(start=5)\n",
            " |      >>> for pos, element in dataset.as_numpy_iterator():\n",
            " |      ...   print(tuple((pos.item(), element.item())))\n",
            " |      (5, 1)\n",
            " |      (6, 2)\n",
            " |      (7, 3)\n",
            " |\n",
            " |      >>> # The (nested) structure of the input dataset determines the\n",
            " |      >>> # structure of elements in the resulting dataset.\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
            " |      >>> dataset = dataset.enumerate()\n",
            " |      >>> for pos, element in dataset.as_numpy_iterator():\n",
            " |      ...   print(tuple((pos.item(), element)))\n",
            " |      (0, array([7, 8], dtype=int32))\n",
            " |      (1, array([ 9, 10], dtype=int32))\n",
            " |\n",
            " |      Args:\n",
            " |        start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
            " |          enumeration.\n",
            " |        name: Optional. A name for the tf.data operations used by `enumerate`.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  filter(self, predicate, name=None) -> 'DatasetV2'\n",
            " |      Filters this dataset according to `predicate`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> dataset = dataset.filter(lambda x: x < 3)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2]\n",
            " |      >>> # `tf.math.equal(x, y)` is required for equality comparison\n",
            " |      >>> def filter_fn(x):\n",
            " |      ...   return tf.math.equal(x, 1)\n",
            " |      >>> dataset = dataset.filter(filter_fn)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1]\n",
            " |\n",
            " |      Args:\n",
            " |        predicate: A function mapping a dataset element to a boolean.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  fingerprint(self)\n",
            " |      Computes the fingerprint of this `Dataset`.\n",
            " |\n",
            " |      If two datasets have the same fingerprint, it is guaranteed that they\n",
            " |      would produce identical elements as long as the content of the upstream\n",
            " |      input files does not change and they produce data deterministically.\n",
            " |\n",
            " |      However, two datasets producing identical values does not always mean they\n",
            " |      would have the same fingerprint due to different graph constructs.\n",
            " |\n",
            " |      In other words, if two datasets have different fingerprints, they could\n",
            " |      still produce identical values.\n",
            " |\n",
            " |      Returns:\n",
            " |        A scalar `tf.Tensor` of type `tf.uint64`.\n",
            " |\n",
            " |  flat_map(self, map_func, name=None) -> 'DatasetV2'\n",
            " |      Maps `map_func` across this dataset and flattens the result.\n",
            " |\n",
            " |      The type signature is:\n",
            " |\n",
            " |      ```\n",
            " |      def flat_map(\n",
            " |        self: Dataset[T],\n",
            " |        map_func: Callable[[T], Dataset[S]]\n",
            " |      ) -> Dataset[S]\n",
            " |      ```\n",
            " |\n",
            " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
            " |      stays the same. For example, to flatten a dataset of batches into a\n",
            " |      dataset of their elements:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(\n",
            " |      ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
            " |      >>> dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |\n",
            " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
            " |      `flat_map` produces the same output as\n",
            " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
            " |\n",
            " |      Args:\n",
            " |        map_func: A function mapping a dataset element to a dataset.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  get_single_element(self, name=None)\n",
            " |      Returns the single element of the `dataset`.\n",
            " |\n",
            " |      The function enables you to use a `tf.data.Dataset` in a stateless\n",
            " |      \"tensor-in tensor-out\" expression, without creating an iterator.\n",
            " |      This facilitates the ease of data transformation on tensors using the\n",
            " |      optimized `tf.data.Dataset` abstraction on top of them.\n",
            " |\n",
            " |      For example, lets consider a `preprocessing_fn` which would take as an\n",
            " |      input the raw features and returns the processed feature along with\n",
            " |      it's label.\n",
            " |\n",
            " |      ```python\n",
            " |      def preprocessing_fn(raw_feature):\n",
            " |        # ... the raw_feature is preprocessed as per the use-case\n",
            " |        return feature\n",
            " |\n",
            " |      raw_features = ...  # input batch of BATCH_SIZE elements.\n",
            " |      dataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n",
            " |                .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n",
            " |                .batch(BATCH_SIZE))\n",
            " |\n",
            " |      processed_features = dataset.get_single_element()\n",
            " |      ```\n",
            " |\n",
            " |      In the above example, the `raw_features` tensor of length=BATCH_SIZE\n",
            " |      was converted to a `tf.data.Dataset`. Next, each of the `raw_feature` was\n",
            " |      mapped using the `preprocessing_fn` and the processed features were\n",
            " |      grouped into a single batch. The final `dataset` contains only one element\n",
            " |      which is a batch of all the processed features.\n",
            " |\n",
            " |      NOTE: The `dataset` should contain only one element.\n",
            " |\n",
            " |      Now, instead of creating an iterator for the `dataset` and retrieving the\n",
            " |      batch of features, the `tf.data.get_single_element()` function is used\n",
            " |      to skip the iterator creation process and directly output the batch of\n",
            " |      features.\n",
            " |\n",
            " |      This can be particularly useful when your tensor transformations are\n",
            " |      expressed as `tf.data.Dataset` operations, and you want to use those\n",
            " |      transformations while serving your model.\n",
            " |\n",
            " |      #### Keras\n",
            " |\n",
            " |      ```python\n",
            " |\n",
            " |      model = ... # A pre-built or custom model\n",
            " |\n",
            " |      class PreprocessingModel(tf.keras.Model):\n",
            " |        def __init__(self, model):\n",
            " |          super().__init__(self)\n",
            " |          self.model = model\n",
            " |\n",
            " |        @tf.function(input_signature=[...])\n",
            " |        def serving_fn(self, data):\n",
            " |          ds = tf.data.Dataset.from_tensor_slices(data)\n",
            " |          ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n",
            " |          ds = ds.batch(batch_size=BATCH_SIZE)\n",
            " |          return tf.argmax(self.model(ds.get_single_element()), axis=-1)\n",
            " |\n",
            " |      preprocessing_model = PreprocessingModel(model)\n",
            " |      your_exported_model_dir = ... # save the model to this path.\n",
            " |      tf.saved_model.save(preprocessing_model, your_exported_model_dir,\n",
            " |                    signatures={'serving_default': preprocessing_model.serving_fn}\n",
            " |                    )\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A nested structure of `tf.Tensor` objects, corresponding to the single\n",
            " |        element of `dataset`.\n",
            " |\n",
            " |      Raises:\n",
            " |        InvalidArgumentError: (at runtime) if `dataset` does not contain exactly\n",
            " |          one element.\n",
            " |\n",
            " |  group_by_window(self, key_func, reduce_func, window_size=None, window_size_func=None, name=None) -> 'DatasetV2'\n",
            " |      Groups windows of elements by key and reduces them.\n",
            " |\n",
            " |      This transformation maps each consecutive element in a dataset to a key\n",
            " |      using `key_func` and groups the elements by key. It then applies\n",
            " |      `reduce_func` to at most `window_size_func(key)` elements matching the same\n",
            " |      key. All except the final window for each key will contain\n",
            " |      `window_size_func(key)` elements; the final window may be smaller.\n",
            " |\n",
            " |      You may provide either a constant `window_size` or a window size determined\n",
            " |      by the key through `window_size_func`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(10)\n",
            " |      >>> window_size = 5\n",
            " |      >>> key_func = lambda x: x%2\n",
            " |      >>> reduce_func = lambda key, dataset: dataset.batch(window_size)\n",
            " |      >>> dataset = dataset.group_by_window(\n",
            " |      ...           key_func=key_func,\n",
            " |      ...           reduce_func=reduce_func,\n",
            " |      ...           window_size=window_size)\n",
            " |      >>> for elem in dataset.as_numpy_iterator():\n",
            " |      ...   print(elem)\n",
            " |      [0 2 4 6 8]\n",
            " |      [1 3 5 7 9]\n",
            " |\n",
            " |      Args:\n",
            " |        key_func: A function mapping a nested structure of tensors (having shapes\n",
            " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
            " |          scalar `tf.int64` tensor.\n",
            " |        reduce_func: A function mapping a key and a dataset of up to `window_size`\n",
            " |          consecutive elements matching that key to another dataset.\n",
            " |        window_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          consecutive elements matching the same key to combine in a single batch,\n",
            " |          which will be passed to `reduce_func`. Mutually exclusive with\n",
            " |          `window_size_func`.\n",
            " |        window_size_func: A function mapping a key to a `tf.int64` scalar\n",
            " |          `tf.Tensor`, representing the number of consecutive elements matching\n",
            " |          the same key to combine in a single batch, which will be passed to\n",
            " |          `reduce_func`. Mutually exclusive with `window_size`.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError: if neither or both of {`window_size`, `window_size_func`} are\n",
            " |          passed.\n",
            " |\n",
            " |  ignore_errors(self, log_warning=False, name=None) -> 'DatasetV2'\n",
            " |      Drops elements that cause errors.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\n",
            " |      >>> dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"\"))\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      InvalidArgumentError: ... Tensor had Inf values\n",
            " |      >>> dataset = dataset.ignore_errors()\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [1.0, 0.5, 0.25]\n",
            " |\n",
            " |      Args:\n",
            " |        log_warning: (Optional.) A bool indicating whether or not ignored errors\n",
            " |          should be logged to stderr. Defaults to `False`.\n",
            " |        name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  interleave(self, map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None, name=None) -> 'DatasetV2'\n",
            " |      Maps `map_func` across this dataset, and interleaves the results.\n",
            " |\n",
            " |      The type signature is:\n",
            " |\n",
            " |      ```\n",
            " |      def interleave(\n",
            " |        self: Dataset[T],\n",
            " |        map_func: Callable[[T], Dataset[S]]\n",
            " |      ) -> Dataset[S]\n",
            " |      ```\n",
            " |\n",
            " |      For example, you can use `Dataset.interleave()` to process many input files\n",
            " |      concurrently:\n",
            " |\n",
            " |      >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
            " |      >>> # from each file.\n",
            " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
            " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
            " |      >>> def parse_fn(filename):\n",
            " |      ...   return tf.data.Dataset.range(10)\n",
            " |      >>> dataset = dataset.interleave(lambda x:\n",
            " |      ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
            " |      ...     cycle_length=4, block_length=16)\n",
            " |\n",
            " |      The `cycle_length` and `block_length` arguments control the order in which\n",
            " |      elements are produced. `cycle_length` controls the number of input elements\n",
            " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
            " |      transformation will handle one input element at a time, and will produce\n",
            " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
            " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
            " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
            " |      producing `block_length` consecutive elements from each iterator, and\n",
            " |      consuming the next input element each time it reaches the end of an\n",
            " |      iterator.\n",
            " |\n",
            " |      For example:\n",
            " |\n",
            " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
            " |      >>> # NOTE: New lines indicate \"block\" boundaries.\n",
            " |      >>> dataset = dataset.interleave(\n",
            " |      ...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
            " |      ...     cycle_length=2, block_length=4)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 1, 1, 1,\n",
            " |       2, 2, 2, 2,\n",
            " |       1, 1,\n",
            " |       2, 2,\n",
            " |       3, 3, 3, 3,\n",
            " |       4, 4, 4, 4,\n",
            " |       3, 3,\n",
            " |       4, 4,\n",
            " |       5, 5, 5, 5,\n",
            " |       5, 5]\n",
            " |\n",
            " |      Note: The order of elements yielded by this transformation is\n",
            " |      deterministic, as long as `map_func` is a pure function and\n",
            " |      `deterministic=True`. If `map_func` contains any stateful operations, the\n",
            " |      order in which that state is accessed is undefined.\n",
            " |\n",
            " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
            " |      `interleave` will use multiple threads to fetch elements. If determinism\n",
            " |      isn't required, it can also improve performance to set\n",
            " |      `deterministic=False`.\n",
            " |\n",
            " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
            " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
            " |      >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
            " |      ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
            " |      ...     deterministic=False)\n",
            " |\n",
            " |      Args:\n",
            " |        map_func: A function that takes a dataset element and returns a\n",
            " |          `tf.data.Dataset`.\n",
            " |        cycle_length: (Optional.) The number of input elements that will be\n",
            " |          processed concurrently. If not set, the tf.data runtime decides what it\n",
            " |          should be based on available CPU. If `num_parallel_calls` is set to\n",
            " |          `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
            " |          the maximum degree of parallelism.\n",
            " |        block_length: (Optional.) The number of consecutive elements to produce\n",
            " |          from each input element before cycling to another input element. If not\n",
            " |          set, defaults to 1.\n",
            " |        num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
            " |          threadpool, which is used to fetch inputs from cycle elements\n",
            " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
            " |          from cycle elements synchronously with no parallelism. If the value\n",
            " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
            " |          calls is set dynamically based on available CPU.\n",
            " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
            " |          boolean is specified (`True` or `False`), it controls the order in which\n",
            " |          the transformation produces elements. If set to `False`, the\n",
            " |          transformation is allowed to yield elements out of order to trade\n",
            " |          determinism for performance. If not specified, the\n",
            " |          `tf.data.Options.deterministic` option (`True` by default) controls the\n",
            " |          behavior.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  map(self, map_func, num_parallel_calls=None, deterministic=None, synchronous=None, use_unbounded_threadpool=False, name=None) -> 'DatasetV2'\n",
            " |      Maps `map_func` across the elements of this dataset.\n",
            " |\n",
            " |      This transformation applies `map_func` to each element of this dataset, and\n",
            " |      returns a new dataset containing the transformed elements, in the same\n",
            " |      order as they appeared in the input. `map_func` can be used to change both\n",
            " |      the values and the structure of a dataset's elements. Supported structure\n",
            " |      constructs are documented\n",
            " |      [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
            " |\n",
            " |      For example, `map` can be used for adding 1 to each element, or projecting a\n",
            " |      subset of element components.\n",
            " |\n",
            " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
            " |      >>> dataset = dataset.map(lambda x: x + 1)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [2, 3, 4, 5, 6]\n",
            " |\n",
            " |      The input signature of `map_func` is determined by the structure of each\n",
            " |      element in this dataset.\n",
            " |\n",
            " |      >>> dataset = Dataset.range(5)\n",
            " |      >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
            " |      >>> # shape and dtype.\n",
            " |      >>> result = dataset.map(lambda x: x + 1)\n",
            " |\n",
            " |      >>> # Each element is a tuple containing two `tf.Tensor` objects.\n",
            " |      >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(\n",
            " |      ...     lambda: elements, (tf.int32, tf.string))\n",
            " |      >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\n",
            " |      >>> # projects out just the first component.\n",
            " |      >>> result = dataset.map(lambda x_int, y_str: x_int)\n",
            " |      >>> [a.item() for a in result.as_numpy_iterator()]\n",
            " |      [1, 2, 3]\n",
            " |\n",
            " |      >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
            " |      >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
            " |      ...               {\"a\": 2, \"b\": \"bar\"},\n",
            " |      ...               {\"a\": 3, \"b\": \"baz\"}])\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(\n",
            " |      ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
            " |      >>> # `map_func` takes a single argument of type `dict` with the same keys\n",
            " |      >>> # as the elements.\n",
            " |      >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
            " |\n",
            " |      The value or values returned by `map_func` determine the structure of each\n",
            " |      element in the returned dataset.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(3)\n",
            " |      >>> # `map_func` returns two `tf.Tensor` objects.\n",
            " |      >>> def g(x):\n",
            " |      ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
            " |      >>> result = dataset.map(g)\n",
            " |      >>> result.element_spec\n",
            " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n",
            " |      >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
            " |      >>> # `tf.Tensor`.\n",
            " |      >>> def h(x):\n",
            " |      ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
            " |      >>> result = dataset.map(h)\n",
            " |      >>> result.element_spec\n",
            " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n",
            " |      >>> # `map_func` can return nested structures.\n",
            " |      >>> def i(x):\n",
            " |      ...   return (37.0, [42, 16]), \"foo\"\n",
            " |      >>> result = dataset.map(i)\n",
            " |      >>> result.element_spec\n",
            " |      ((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
            " |        TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
            " |       TensorSpec(shape=(), dtype=tf.string, name=None))\n",
            " |\n",
            " |      `map_func` can accept as arguments and return any type of dataset element.\n",
            " |\n",
            " |      Note that irrespective of the context in which `map_func` is defined (eager\n",
            " |      vs. graph), tf.data traces the function and executes it as a graph. To use\n",
            " |      Python code inside of the function you have a few options:\n",
            " |\n",
            " |      1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
            " |      computation. The downside of this approach is that AutoGraph can convert\n",
            " |      some but not all Python code.\n",
            " |\n",
            " |      2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
            " |      will generally result in worse performance than 1). For example:\n",
            " |\n",
            " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
            " |      >>> # transform a string tensor to upper case string using a Python function\n",
            " |      >>> def upper_case_fn(t: tf.Tensor):\n",
            " |      ...   return t.numpy().decode('utf-8').upper()\n",
            " |      >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
            " |      ...           inp=[x], Tout=tf.string))\n",
            " |      >>> list(d.as_numpy_iterator())\n",
            " |      [b'HELLO', b'WORLD']\n",
            " |\n",
            " |      3) Use `tf.numpy_function`, which also allows you to write arbitrary\n",
            " |      Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\n",
            " |      `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\n",
            " |      For example:\n",
            " |\n",
            " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
            " |      >>> def upper_case_fn(t: np.ndarray):\n",
            " |      ...   return t.decode('utf-8').upper()\n",
            " |      >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
            " |      ...           inp=[x], Tout=tf.string))\n",
            " |      >>> list(d.as_numpy_iterator())\n",
            " |      [b'HELLO', b'WORLD']\n",
            " |\n",
            " |      Note that the use of `tf.numpy_function` and `tf.py_function`\n",
            " |      in general precludes the possibility of executing user-defined\n",
            " |      transformations in parallel (because of Python GIL).\n",
            " |\n",
            " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
            " |      `map` will use multiple threads to process elements. If deterministic order\n",
            " |      isn't required, it can also improve performance to set\n",
            " |      `deterministic=False`.\n",
            " |\n",
            " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
            " |      >>> dataset = dataset.map(lambda x: x + 1,\n",
            " |      ...     num_parallel_calls=tf.data.AUTOTUNE,\n",
            " |      ...     deterministic=False)\n",
            " |\n",
            " |      The order of elements yielded by this transformation is deterministic if\n",
            " |      `deterministic=True`. If `map_func` contains stateful operations and\n",
            " |      `num_parallel_calls > 1`, the order in which that state is accessed is\n",
            " |      undefined, so the values of output elements may not be deterministic\n",
            " |      regardless of the `deterministic` flag value.\n",
            " |\n",
            " |      Args:\n",
            " |        map_func: A function mapping a dataset element to another dataset element.\n",
            " |        num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
            " |          representing the number elements to process asynchronously in parallel.\n",
            " |          If the value `tf.data.AUTOTUNE` is used, then the number of parallel\n",
            " |          calls is set dynamically based on available CPU. If not specified, the\n",
            " |          `tf.data.Options.experimental_optimization.map_parallelization` option\n",
            " |          (`True` by default) controls whether the map will run as with\n",
            " |          `tf.data.AUTOTUNE` or run sequentially.\n",
            " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
            " |          boolean is specified (`True` or `False`), it controls the order in which\n",
            " |          the transformation produces elements. If set to `False`, the\n",
            " |          transformation is allowed to yield elements out of order to trade\n",
            " |          determinism for performance. If not specified, the\n",
            " |          `tf.data.Options.deterministic` option (`True` by default) controls the\n",
            " |          behavior.\n",
            " |        synchronous: (Optional.) Whether to force the map transformation to run\n",
            " |          synchronously. This only matters when\n",
            " |          `options.experimental_optimization.map_parallelization=True`. That\n",
            " |          option would normally change the map to run with\n",
            " |          `num_parallel_calls=tf.data.AUTOTUNE`, but if `synchronous=True` is\n",
            " |          specified, the map will not be parallelized at all. This is useful for\n",
            " |          saving memory, since even setting `num_parallel_calls=1` will cause one\n",
            " |          batch to be buffered, while with `synchronous=True` the map\n",
            " |          transformation doesn't buffer anything.\n",
            " |        use_unbounded_threadpool: (Optional.) By default, map functions run in a\n",
            " |          limited threadpool based on the number of cores on the machine. This\n",
            " |          efficient for CPU-heavy processing, but if the map function performs IO\n",
            " |          it is better to use an unbounded threadpool by setting it to `True`. It\n",
            " |          is `False` by default.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  options(self)\n",
            " |      Returns the options for this dataset and its inputs.\n",
            " |\n",
            " |      Returns:\n",
            " |        A `tf.data.Options` object representing the dataset options.\n",
            " |\n",
            " |  padded_batch(self, batch_size, padded_shapes=None, padding_values=None, drop_remainder=False, name=None) -> 'DatasetV2'\n",
            " |      Combines consecutive elements of this dataset into padded batches.\n",
            " |\n",
            " |      This transformation combines multiple consecutive elements of the input\n",
            " |      dataset into a single element.\n",
            " |\n",
            " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
            " |      have an additional outer dimension, which will be `batch_size` (or\n",
            " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
            " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
            " |      your program depends on the batches having the same outer dimension, you\n",
            " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
            " |      batch from being produced.\n",
            " |\n",
            " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
            " |      different shapes, and this transformation will pad each component to the\n",
            " |      respective shape in `padded_shapes`. The `padded_shapes` argument\n",
            " |      determines the resulting shape for each dimension of each component in an\n",
            " |      output element:\n",
            " |\n",
            " |      * If the dimension is a constant, the component will be padded out to that\n",
            " |        length in that dimension.\n",
            " |      * If the dimension is unknown, the component will be padded out to the\n",
            " |        maximum length of all elements in that dimension.\n",
            " |\n",
            " |      >>> A = (tf.data.Dataset\n",
            " |      ...      .range(1, 5, output_type=tf.int32)\n",
            " |      ...      .map(lambda x: tf.fill([x], x)))\n",
            " |      >>> # Pad to the smallest per-batch size that fits all elements.\n",
            " |      >>> B = A.padded_batch(2)\n",
            " |      >>> for element in B.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      [[1 0]\n",
            " |       [2 2]]\n",
            " |      [[3 3 3 0]\n",
            " |       [4 4 4 4]]\n",
            " |      >>> # Pad to a fixed size.\n",
            " |      >>> C = A.padded_batch(2, padded_shapes=5)\n",
            " |      >>> for element in C.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      [[1 0 0 0 0]\n",
            " |       [2 2 0 0 0]]\n",
            " |      [[3 3 3 0 0]\n",
            " |       [4 4 4 4 0]]\n",
            " |      >>> # Pad with a custom value.\n",
            " |      >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
            " |      >>> for element in D.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      [[ 1 -1 -1 -1 -1]\n",
            " |       [ 2  2 -1 -1 -1]]\n",
            " |      [[ 3  3  3 -1 -1]\n",
            " |       [ 4  4  4  4 -1]]\n",
            " |      >>> # Components of nested elements can be padded independently.\n",
            " |      >>> elements = [([1, 2, 3], [10]),\n",
            " |      ...             ([4, 5], [11, 12])]\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(\n",
            " |      ...     lambda: iter(elements), (tf.int32, tf.int32))\n",
            " |      >>> # Pad the first component of the tuple to length 4, and the second\n",
            " |      >>> # component to the smallest size that fits.\n",
            " |      >>> dataset = dataset.padded_batch(2,\n",
            " |      ...     padded_shapes=([4], [None]),\n",
            " |      ...     padding_values=(-1, 100))\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n",
            " |        array([[ 10, 100], [ 11,  12]], dtype=int32))]\n",
            " |      >>> # Pad with a single value and multiple components.\n",
            " |      >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
            " |      >>> for element in E.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      (array([[ 1, -1],\n",
            " |             [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
            " |             [ 2,  2]], dtype=int32))\n",
            " |      (array([[ 3,  3,  3, -1],\n",
            " |             [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
            " |             [ 4,  4,  4,  4]], dtype=int32))\n",
            " |\n",
            " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
            " |      elements that may have different shapes into a `tf.sparse.SparseTensor`.\n",
            " |\n",
            " |      Args:\n",
            " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          consecutive elements of this dataset to combine in a single batch.\n",
            " |        padded_shapes: (Optional.) A (nested) structure of `tf.TensorShape` or\n",
            " |          `tf.int64` vector tensor-like objects representing the shape to which\n",
            " |          the respective component of each input element should be padded prior\n",
            " |          to batching. Any unknown dimensions will be padded to the maximum size\n",
            " |          of that dimension in each batch. If unset, all dimensions of all\n",
            " |          components are padded to the maximum size in the batch. `padded_shapes`\n",
            " |          must be set if any component has an unknown rank.\n",
            " |        padding_values: (Optional.) A (nested) structure of scalar-shaped\n",
            " |          `tf.Tensor`, representing the padding values to use for the respective\n",
            " |          components. None represents that the (nested) structure should be padded\n",
            " |          with default values.  Defaults are `0` for numeric types and the empty\n",
            " |          string for string types. The `padding_values` should have the same\n",
            " |          (nested) structure as the input dataset. If `padding_values` is a single\n",
            " |          element and the input dataset has multiple components, then the same\n",
            " |          `padding_values` will be used to pad every component of the dataset.\n",
            " |          If `padding_values` is a scalar, then its value will be broadcasted\n",
            " |          to match the shape of each component.\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last batch should be dropped in the case it has fewer than\n",
            " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
            " |          batch.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError: If a component has an unknown rank, and the `padded_shapes`\n",
            " |          argument is not set.\n",
            " |        TypeError: If a component is of an unsupported type. The list of supported\n",
            " |          types is documented in\n",
            " |          https://www.tensorflow.org/guide/data#dataset_structure.\n",
            " |\n",
            " |  prefetch(self, buffer_size, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
            " |\n",
            " |      Most dataset input pipelines should end with a call to `prefetch`. This\n",
            " |      allows later elements to be prepared while the current element is being\n",
            " |      processed. This often improves latency and throughput, at the cost of\n",
            " |      using additional memory to store prefetched elements.\n",
            " |\n",
            " |      Note: Like other `Dataset` methods, prefetch operates on the\n",
            " |      elements of the input dataset. It has no concept of examples vs. batches.\n",
            " |      `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
            " |      while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
            " |      (2 batches, of 20 examples each).\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(3)\n",
            " |      >>> dataset = dataset.prefetch(2)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 2]\n",
            " |\n",
            " |      Args:\n",
            " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
            " |          number of elements that will be buffered when prefetching. If the value\n",
            " |          `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned.\n",
            " |        name: Optional. A name for the tf.data transformation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  ragged_batch(self, batch_size, drop_remainder=False, row_splits_dtype=tf.int64, name=None) -> 'DatasetV2'\n",
            " |      Combines consecutive elements of this dataset into `tf.RaggedTensor`s.\n",
            " |\n",
            " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
            " |      have an additional outer dimension, which will be `batch_size` (or\n",
            " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
            " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
            " |      your program depends on the batches having the same outer dimension, you\n",
            " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
            " |      batch from being produced.\n",
            " |\n",
            " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
            " |      different shapes:\n",
            " |\n",
            " |      *  If an input element is a `tf.Tensor` whose static `tf.TensorShape` is\n",
            " |      fully defined, then it is batched as normal.\n",
            " |      *  If an input element is a `tf.Tensor` whose static `tf.TensorShape`\n",
            " |      contains one or more axes with unknown size (i.e., `shape[i]=None`), then\n",
            " |      the output will contain a `tf.RaggedTensor` that is ragged up to any of such\n",
            " |      dimensions.\n",
            " |      *  If an input element is a `tf.RaggedTensor` or any other type, then it is\n",
            " |      batched as normal.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(6)\n",
            " |      >>> dataset = dataset.map(lambda x: tf.range(x))\n",
            " |      >>> dataset.element_spec.shape\n",
            " |      TensorShape([None])\n",
            " |      >>> dataset = dataset.ragged_batch(2)\n",
            " |      >>> for batch in dataset:\n",
            " |      ...   print(batch)\n",
            " |      <tf.RaggedTensor [[], [0]]>\n",
            " |      <tf.RaggedTensor [[0, 1], [0, 1, 2]]>\n",
            " |      <tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>\n",
            " |\n",
            " |      Args:\n",
            " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          consecutive elements of this dataset to combine in a single batch.\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last batch should be dropped in the case it has fewer than\n",
            " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
            " |          batch.\n",
            " |        row_splits_dtype: The dtype that should be used for the `row_splits` of\n",
            " |          any new ragged tensors.  Existing `tf.RaggedTensor` elements do not have\n",
            " |          their row_splits dtype changed.\n",
            " |        name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  rebatch(self, batch_size, drop_remainder=False, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` that rebatches the elements from this dataset.\n",
            " |\n",
            " |      `rebatch(N)` is functionally equivalent to `unbatch().batch(N)`, but is\n",
            " |      more efficient, performing one copy instead of two.\n",
            " |\n",
            " |      >>> ds = tf.data.Dataset.range(6)\n",
            " |      >>> ds = ds.batch(2)\n",
            " |      >>> ds = ds.rebatch(3)\n",
            " |      >>> list(ds.as_numpy_iterator())\n",
            " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
            " |\n",
            " |      >>> ds = tf.data.Dataset.range(7)\n",
            " |      >>> ds = ds.batch(4)\n",
            " |      >>> ds = ds.rebatch(3)\n",
            " |      >>> list(ds.as_numpy_iterator())\n",
            " |      [array([0, 1, 2]), array([3, 4, 5]), array([6])]\n",
            " |\n",
            " |      >>> ds = tf.data.Dataset.range(7)\n",
            " |      >>> ds = ds.batch(2)\n",
            " |      >>> ds = ds.rebatch(3, drop_remainder=True)\n",
            " |      >>> list(ds.as_numpy_iterator())\n",
            " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
            " |\n",
            " |      If the `batch_size` argument is a list, `rebatch` cycles through the list\n",
            " |      to determine the size of each batch.\n",
            " |\n",
            " |      >>> ds = tf.data.Dataset.range(8)\n",
            " |      >>> ds = ds.batch(4)\n",
            " |      >>> ds = ds.rebatch([2, 1, 1])\n",
            " |      >>> list(ds.as_numpy_iterator())\n",
            " |      [array([0, 1]), array([2]), array([3]), array([4, 5]), array([6]),\n",
            " |      array([7])]\n",
            " |\n",
            " |      Args:\n",
            " |        batch_size: A `tf.int64` scalar or vector, representing the size of\n",
            " |          batches to produce. If this argument is a vector, these values are\n",
            " |          cycled through in round robin fashion.\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last batch should be dropped in the case it has fewer than\n",
            " |          `batch_size[cycle_index]` elements; the default behavior is not to drop\n",
            " |          the smaller batch.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A `Dataset` of scalar `dtype` elements.\n",
            " |\n",
            " |  reduce(self, initial_state, reduce_func, name=None)\n",
            " |      Reduces the input dataset to a single element.\n",
            " |\n",
            " |      The transformation calls `reduce_func` successively on every element of\n",
            " |      the input dataset until the dataset is exhausted, aggregating information in\n",
            " |      its internal state. The `initial_state` argument is used for the initial\n",
            " |      state and the final state is returned as the result.\n",
            " |\n",
            " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x +\n",
            " |      ...   1).numpy().item()\n",
            " |      5\n",
            " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x +\n",
            " |      ...   y).numpy().item()\n",
            " |      10\n",
            " |\n",
            " |      Args:\n",
            " |        initial_state: An element representing the initial state of the\n",
            " |          transformation.\n",
            " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
            " |          `new_state`. It must take two arguments and return a new element The\n",
            " |          structure of `new_state` must match the structure of `initial_state`.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A dataset element corresponding to the final state of the transformation.\n",
            " |\n",
            " |  rejection_resample(self, class_func, target_dist, initial_dist=None, seed=None, name=None) -> 'DatasetV2'\n",
            " |      Resamples elements to reach a target distribution.\n",
            " |\n",
            " |      Note: This implementation can reject **or repeat** elements in order to\n",
            " |      reach the `target_dist`. So, in some cases, the output `Dataset` may be\n",
            " |      larger than the input `Dataset`.\n",
            " |\n",
            " |      >>> initial_dist = [0.6, 0.4]\n",
            " |      >>> n = 1000\n",
            " |      >>> elems = np.random.choice(len(initial_dist), size=n, p=initial_dist)\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(elems)\n",
            " |      >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n",
            " |\n",
            " |      Following from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4.\n",
            " |\n",
            " |      >>> target_dist = [0.5, 0.5]\n",
            " |      >>> dataset = dataset.rejection_resample(\n",
            " |      ...    class_func=lambda x: x,\n",
            " |      ...    target_dist=target_dist,\n",
            " |      ...    initial_dist=initial_dist)\n",
            " |      >>> dataset = dataset.map(lambda class_func_result, data: data)\n",
            " |      >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n",
            " |\n",
            " |      Following from `target_dist`, `zero` is ~0.5 and `one` is ~0.5.\n",
            " |\n",
            " |      Args:\n",
            " |        class_func: A function mapping an element of the input dataset to a scalar\n",
            " |          `tf.int32` tensor. Values should be in `[0, num_classes)`.\n",
            " |        target_dist: A floating point type tensor, shaped `[num_classes]`.\n",
            " |        initial_dist: (Optional.)  A floating point type tensor, shaped\n",
            " |          `[num_classes]`.  If not provided, the true class distribution is\n",
            " |          estimated live in a streaming fashion.\n",
            " |        seed: (Optional.) Python integer seed for the resampler.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  repeat(self, count=None, name=None) -> 'DatasetV2'\n",
            " |      Repeats this dataset so each original value is seen `count` times.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> dataset = dataset.repeat(3)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
            " |\n",
            " |      Note: If the input dataset depends on global state (e.g. a random number\n",
            " |      generator) or its output is non-deterministic (e.g. because of upstream\n",
            " |      `shuffle`), then different repetitions may produce different elements.\n",
            " |\n",
            " |      Args:\n",
            " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
            " |          number of times the dataset should be repeated. The default behavior (if\n",
            " |          `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  save(self, path, compression=None, shard_func=None, checkpoint_args=None)\n",
            " |      Saves the content of the given dataset.\n",
            " |\n",
            " |        Example usage:\n",
            " |\n",
            " |        >>> import tempfile\n",
            " |        >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n",
            " |        >>> # Save a dataset\n",
            " |        >>> dataset = tf.data.Dataset.range(2)\n",
            " |        >>> dataset.save(path)\n",
            " |        >>> new_dataset = tf.data.Dataset.load(path)\n",
            " |        >>> for elem in new_dataset:\n",
            " |        ...   print(elem)\n",
            " |        tf.Tensor(0, shape=(), dtype=int64)\n",
            " |        tf.Tensor(1, shape=(), dtype=int64)\n",
            " |\n",
            " |        The saved dataset is saved in multiple file \"shards\". By default, the\n",
            " |        dataset output is divided to shards in a round-robin fashion but custom\n",
            " |        sharding can be specified via the `shard_func` function. For example, you\n",
            " |        can save the dataset to using a single shard as follows:\n",
            " |\n",
            " |        ```python\n",
            " |        dataset = make_dataset()\n",
            " |        def custom_shard_func(element):\n",
            " |          return np.int64(0)\n",
            " |        dataset.save(\n",
            " |            path=\"/path/to/data\", ..., shard_func=custom_shard_func)\n",
            " |        ```\n",
            " |\n",
            " |        To enable checkpointing, pass in `checkpoint_args` to the `save` method\n",
            " |        as follows:\n",
            " |\n",
            " |        ```python\n",
            " |        dataset = tf.data.Dataset.range(100)\n",
            " |        save_dir = \"...\"\n",
            " |        checkpoint_prefix = \"...\"\n",
            " |        step_counter = tf.Variable(0, trainable=False)\n",
            " |        checkpoint_args = {\n",
            " |          \"checkpoint_interval\": 50,\n",
            " |          \"step_counter\": step_counter,\n",
            " |          \"directory\": checkpoint_prefix,\n",
            " |          \"max_to_keep\": 20,\n",
            " |        }\n",
            " |        dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)\n",
            " |        ```\n",
            " |\n",
            " |        NOTE: The directory layout and file format used for saving the dataset is\n",
            " |        considered an implementation detail and may change. For this reason,\n",
            " |        datasets saved through `tf.data.Dataset.save` should only be consumed\n",
            " |        through `tf.data.Dataset.load`, which is guaranteed to be\n",
            " |        backwards compatible.\n",
            " |\n",
            " |      Args:\n",
            " |       path: Required. A directory to use for saving the dataset.\n",
            " |       compression: Optional. The algorithm to use to compress data when writing\n",
            " |            it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\n",
            " |       shard_func: Optional. A function to control the mapping of dataset\n",
            " |            elements to file shards. The function is expected to map elements of\n",
            " |            the input dataset to int64 shard IDs. If present, the function will be\n",
            " |            traced and executed as graph computation.\n",
            " |       checkpoint_args: Optional args for checkpointing which will be passed into\n",
            " |            the `tf.train.CheckpointManager`. If `checkpoint_args` are not\n",
            " |            specified, then checkpointing will not be performed. The `save()`\n",
            " |            implementation creates a `tf.train.Checkpoint` object internally, so\n",
            " |            users should not set the `checkpoint` argument in `checkpoint_args`.\n",
            " |\n",
            " |      Returns:\n",
            " |        An operation which when executed performs the save. When writing\n",
            " |        checkpoints, returns None. The return value is useful in unit tests.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError if `checkpoint` is passed into `checkpoint_args`.\n",
            " |\n",
            " |  scan(self, initial_state, scan_func, name=None) -> 'DatasetV2'\n",
            " |      A transformation that scans a function across an input dataset.\n",
            " |\n",
            " |      This transformation is a stateful relative of `tf.data.Dataset.map`.\n",
            " |      In addition to mapping `scan_func` across the elements of the input dataset,\n",
            " |      `scan()` accumulates one or more state tensors, whose initial values are\n",
            " |      `initial_state`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(10)\n",
            " |      >>> initial_state = tf.constant(0, dtype=tf.int64)\n",
            " |      >>> scan_func = lambda state, i: (state + i, state + i)\n",
            " |      >>> dataset = dataset.scan(initial_state=initial_state, scan_func=scan_func)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n",
            " |\n",
            " |      Args:\n",
            " |        initial_state: A nested structure of tensors, representing the initial\n",
            " |          state of the accumulator.\n",
            " |        scan_func: A function that maps `(old_state, input_element)` to\n",
            " |          `(new_state, output_element)`. It must take two arguments and return a\n",
            " |          pair of nested structures of tensors. The `new_state` must match the\n",
            " |          structure of `initial_state`.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  shard(self, num_shards, index, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
            " |\n",
            " |      `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\n",
            " |      contain all elements of A whose index mod n = i.\n",
            " |\n",
            " |      >>> A = tf.data.Dataset.range(10)\n",
            " |      >>> B = A.shard(num_shards=3, index=0)\n",
            " |      >>> [a.item() for a in B.as_numpy_iterator()]\n",
            " |      [0, 3, 6, 9]\n",
            " |      >>> C = A.shard(num_shards=3, index=1)\n",
            " |      >>> [a.item() for a in C.as_numpy_iterator()]\n",
            " |      [1, 4, 7]\n",
            " |      >>> D = A.shard(num_shards=3, index=2)\n",
            " |      >>> [a.item() for a in D.as_numpy_iterator()]\n",
            " |      [2, 5, 8]\n",
            " |\n",
            " |      This dataset operator is very useful when running distributed training, as\n",
            " |      it allows each worker to read a unique subset.\n",
            " |\n",
            " |      When reading a single input file, you can shard elements as follows:\n",
            " |\n",
            " |      ```python\n",
            " |      d = tf.data.TFRecordDataset(input_file)\n",
            " |      d = d.shard(num_workers, worker_index)\n",
            " |      d = d.repeat(num_epochs)\n",
            " |      d = d.shuffle(shuffle_buffer_size)\n",
            " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
            " |      ```\n",
            " |\n",
            " |      Important caveats:\n",
            " |\n",
            " |      - Be sure to shard before you use any randomizing operator (such as\n",
            " |        shuffle).\n",
            " |      - Generally it is best if the shard operator is used early in the dataset\n",
            " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
            " |        before converting the dataset to input samples. This avoids reading every\n",
            " |        file on every worker. The following is an example of an efficient\n",
            " |        sharding strategy within a complete pipeline:\n",
            " |\n",
            " |      ```python\n",
            " |      d = Dataset.list_files(pattern, shuffle=False)\n",
            " |      d = d.shard(num_workers, worker_index)\n",
            " |      d = d.repeat(num_epochs)\n",
            " |      d = d.shuffle(shuffle_buffer_size)\n",
            " |      d = d.interleave(tf.data.TFRecordDataset,\n",
            " |                       cycle_length=num_readers, block_length=1)\n",
            " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          shards operating in parallel.\n",
            " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
            " |\n",
            " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
            " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
            " |          placeholder tensor bypasses the early checking, and will instead result\n",
            " |          in an error during a session.run call.)\n",
            " |\n",
            " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=True, name=None) -> 'DatasetV2'\n",
            " |      Randomly shuffles the elements of this dataset.\n",
            " |\n",
            " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
            " |      samples elements from this buffer, replacing the selected elements with new\n",
            " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
            " |      full size of the dataset is required.\n",
            " |\n",
            " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
            " |      set to 1,000, then `shuffle` will initially select a random element from\n",
            " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
            " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
            " |      maintaining the 1,000 element buffer.\n",
            " |\n",
            " |      `reshuffle_each_iteration` controls whether the shuffle order should be\n",
            " |      different for each epoch. However you should avoid using\n",
            " |      `shuffle(reshuffle_each_iteration=True)`, then `take` and `skip` to split\n",
            " |      a dataset into training and test sets, which would lead to data leakage (as\n",
            " |      the entire dataset would be re-shuffled then re-split after each epoch).\n",
            " |      Please use the `tf.keras.utils.split_dataset` method instead. In TF 1.X,\n",
            " |      the idiomatic way to create epochs was through the `repeat` transformation:\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = tf.data.Dataset.range(3)\n",
            " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
            " |      dataset = dataset.repeat(2)\n",
            " |      # [1, 0, 2, 1, 2, 0]\n",
            " |\n",
            " |      dataset = tf.data.Dataset.range(3)\n",
            " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
            " |      dataset = dataset.repeat(2)\n",
            " |      # [1, 0, 2, 1, 0, 2]\n",
            " |      ```\n",
            " |\n",
            " |      In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
            " |      possible to also create epochs through Python iteration:\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = tf.data.Dataset.range(3)\n",
            " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [1, 0, 2]\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [1, 2, 0]\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = tf.data.Dataset.range(3)\n",
            " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [1, 0, 2]\n",
            " |      list(dataset.as_numpy_iterator())\n",
            " |      # [1, 0, 2]\n",
            " |      ```\n",
            " |\n",
            " |      #### Fully shuffling all the data\n",
            " |\n",
            " |      To shuffle an entire dataset, set `buffer_size=dataset.cardinality()`. This\n",
            " |      is equivalent to setting the `buffer_size` equal to the number of elements\n",
            " |      in the dataset, resulting in uniform shuffle.\n",
            " |\n",
            " |      Note: `shuffle(dataset.cardinality())` loads the full dataset into memory so\n",
            " |      that it can be shuffled. This will cause a memory overflow (OOM) error if\n",
            " |      the dataset is too large, so full-shuffle should only be used for datasets\n",
            " |      that are known to fit in the memory, such as datasets of filenames or other\n",
            " |      small datasets.\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = tf.data.Dataset.range(20)\n",
            " |      dataset = dataset.shuffle(dataset.cardinality())\n",
            " |      # [18, 4, 9, 2, 17, 8, 5, 10, 0, 6, 16, 3, 19, 7, 14, 11, 15, 13, 12, 1]\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        buffer_size: An int or `tf.int64` scalar `tf.Tensor`, representing the\n",
            " |          number of elements from this dataset from which the new dataset will\n",
            " |          sample. To uniformly shuffle the entire dataset, use\n",
            " |          `buffer_size=dataset.cardinality()`.\n",
            " |        seed: (Optional.) An int or `tf.int64` scalar `tf.Tensor`, representing\n",
            " |          the random seed that will be used to create the distribution. See\n",
            " |          `tf.random.set_seed` for behavior.\n",
            " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
            " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
            " |          iterated over. (Defaults to `True`.)\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  skip(self, count, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(10)\n",
            " |      >>> dataset = dataset.skip(7)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [7, 8, 9]\n",
            " |\n",
            " |      Args:\n",
            " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          elements of this dataset that should be skipped to form the new dataset.\n",
            " |          If `count` is greater than the size of this dataset, the new dataset\n",
            " |          will contain no elements.  If `count` is -1, skips the entire dataset.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  snapshot(self, path, compression='AUTO', reader_func=None, shard_func=None, name=None) -> 'DatasetV2'\n",
            " |      API to persist the output of the input dataset.\n",
            " |\n",
            " |      The snapshot API allows users to transparently persist the output of their\n",
            " |      preprocessing pipeline to disk, and materialize the pre-processed data on a\n",
            " |      different training run.\n",
            " |\n",
            " |      This API enables repeated preprocessing steps to be consolidated, and allows\n",
            " |      re-use of already processed data, trading off disk storage and network\n",
            " |      bandwidth for freeing up more valuable CPU resources and accelerator compute\n",
            " |      time.\n",
            " |\n",
            " |      https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md\n",
            " |      has detailed design documentation of this feature.\n",
            " |\n",
            " |      Users can specify various options to control the behavior of snapshot,\n",
            " |      including how snapshots are read from and written to by passing in\n",
            " |      user-defined functions to the `reader_func` and `shard_func` parameters.\n",
            " |\n",
            " |      `shard_func` is a user specified function that maps input elements to\n",
            " |      snapshot shards.\n",
            " |\n",
            " |      Users may want to specify this function to control how snapshot files should\n",
            " |      be written to disk. Below is an example of how a potential `shard_func`\n",
            " |      could be written.\n",
            " |\n",
            " |      ```python\n",
            " |      dataset = ...\n",
            " |      dataset = dataset.enumerate()\n",
            " |      dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n",
            " |          shard_func=lambda x, y: x % NUM_SHARDS, ...)\n",
            " |      dataset = dataset.map(lambda x, y: y)\n",
            " |      ```\n",
            " |\n",
            " |      `reader_func` is a user specified function that accepts a single argument:\n",
            " |      (1) a Dataset of Datasets, each representing a \"split\" of elements of the\n",
            " |      original dataset. The cardinality of the input dataset matches the\n",
            " |      number of the shards specified in the `shard_func` (see above). The function\n",
            " |      should return a Dataset of elements of the original dataset.\n",
            " |\n",
            " |      Users may want specify this function to control how snapshot files should be\n",
            " |      read from disk, including the amount of shuffling and parallelism.\n",
            " |\n",
            " |      Here is an example of a standard reader function a user can define. This\n",
            " |      function enables both dataset shuffling and parallel reading of datasets:\n",
            " |\n",
            " |      ```python\n",
            " |      def user_reader_func(datasets):\n",
            " |        # shuffle the datasets splits\n",
            " |        datasets = datasets.shuffle(NUM_CORES)\n",
            " |        # read datasets in parallel and interleave their elements\n",
            " |        return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n",
            " |\n",
            " |      dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n",
            " |          reader_func=user_reader_func)\n",
            " |      ```\n",
            " |\n",
            " |      By default, snapshot parallelizes reads by the number of cores available on\n",
            " |      the system, but will not attempt to shuffle the data.\n",
            " |\n",
            " |      Args:\n",
            " |        path: Required. A directory to use for storing / loading the snapshot to /\n",
            " |          from.\n",
            " |        compression: Optional. The type of compression to apply to the snapshot\n",
            " |          written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None.\n",
            " |          Defaults to `AUTO`, which attempts to pick an appropriate compression\n",
            " |          algorithm for the dataset.\n",
            " |        reader_func: Optional. A function to control how to read data from\n",
            " |          snapshot shards.\n",
            " |        shard_func: Optional. A function to control how to shard data when writing\n",
            " |          a snapshot.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  sparse_batch(self, batch_size, row_shape, name=None) -> 'DatasetV2'\n",
            " |      Combines consecutive elements into `tf.sparse.SparseTensor`s.\n",
            " |\n",
            " |      Like `Dataset.padded_batch()`, this transformation combines multiple\n",
            " |      consecutive elements of the dataset, which might have different\n",
            " |      shapes, into a single element. The resulting element has three\n",
            " |      components (`indices`, `values`, and `dense_shape`), which\n",
            " |      comprise a `tf.sparse.SparseTensor` that represents the same data. The\n",
            " |      `row_shape` represents the dense shape of each row in the\n",
            " |      resulting `tf.sparse.SparseTensor`, to which the effective batch size is\n",
            " |      prepended. For example:\n",
            " |\n",
            " |      ```python\n",
            " |      # NOTE: The following examples use `{ ... }` to represent the\n",
            " |      # contents of a dataset.\n",
            " |      a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n",
            " |\n",
            " |      a.apply(tf.data.experimental.dense_to_sparse_batch(\n",
            " |          batch_size=2, row_shape=[6])) ==\n",
            " |      {\n",
            " |          ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices\n",
            " |           ['a', 'b', 'c', 'a', 'b'],                 # values\n",
            " |           [2, 6]),                                   # dense_shape\n",
            " |          ([[0, 0], [0, 1], [0, 2], [0, 3]],\n",
            " |           ['a', 'b', 'c', 'd'],\n",
            " |           [1, 6])\n",
            " |      }\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          consecutive elements of this dataset to combine in a single batch.\n",
            " |        row_shape: A `tf.TensorShape` or `tf.int64` vector tensor-like object\n",
            " |          representing the equivalent dense shape of a row in the resulting\n",
            " |          `tf.sparse.SparseTensor`. Each element of this dataset must have the\n",
            " |          same rank as `row_shape`, and must have size less than or equal to\n",
            " |          `row_shape` in each dimension.\n",
            " |        name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  take(self, count, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(10)\n",
            " |      >>> dataset = dataset.take(3)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 2]\n",
            " |\n",
            " |      Args:\n",
            " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
            " |          elements of this dataset that should be taken to form the new dataset.\n",
            " |          If `count` is -1, or if `count` is greater than the size of this\n",
            " |          dataset, the new dataset will contain all elements of this dataset.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  take_while(self, predicate, name=None) -> 'DatasetV2'\n",
            " |      A transformation that stops dataset iteration based on a `predicate`.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(10)\n",
            " |      >>> dataset = dataset.take_while(lambda x: x < 5)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |\n",
            " |      Args:\n",
            " |        predicate: A function that maps a nested structure of tensors (having\n",
            " |          shapes and types defined by `self.output_shapes` and\n",
            " |          `self.output_types`) to a scalar `tf.bool` tensor.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  unbatch(self, name=None) -> 'DatasetV2'\n",
            " |      Splits elements of a dataset into multiple elements.\n",
            " |\n",
            " |      For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
            " |      where `B` may vary for each input element, then for each element in the\n",
            " |      dataset, the unbatched dataset will contain `B` consecutive elements\n",
            " |      of shape `[a0, a1, ...]`.\n",
            " |\n",
            " |      >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
            " |      >>> dataset = dataset.unbatch()\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
            " |\n",
            " |      Note: `unbatch` requires a data copy to slice up the batched tensor into\n",
            " |      smaller, unbatched tensors. When optimizing performance, try to avoid\n",
            " |      unnecessary usage of `unbatch`.\n",
            " |\n",
            " |      Args:\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  unique(self, name=None) -> 'DatasetV2'\n",
            " |      A transformation that discards duplicate elements of a `Dataset`.\n",
            " |\n",
            " |      Use this transformation to produce a dataset that contains one instance of\n",
            " |      each unique element in the input. For example:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\n",
            " |      >>> dataset = dataset.unique()\n",
            " |      >>> sorted([a.item() for a in dataset.as_numpy_iterator()])\n",
            " |      [1, 2, 37]\n",
            " |\n",
            " |      Note: This transformation only supports datasets which fit into memory\n",
            " |      and have elements of either `tf.int32`, `tf.int64` or `tf.string` type.\n",
            " |\n",
            " |      Args:\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  window(self, size, shift=None, stride=1, drop_remainder=False, name=None) -> 'DatasetV2'\n",
            " |      Returns a dataset of \"windows\".\n",
            " |\n",
            " |      Each \"window\" is a dataset that contains a subset of elements of the\n",
            " |      input dataset. These are finite datasets of size `size` (or possibly fewer\n",
            " |      if there are not enough input elements to fill the window and\n",
            " |      `drop_remainder` evaluates to `False`).\n",
            " |\n",
            " |      For example:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(7).window(3)\n",
            " |      >>> for window in dataset:\n",
            " |      ...   print(window)\n",
            " |      <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
            " |      <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
            " |      <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
            " |\n",
            " |      Since windows are datasets, they can be iterated over:\n",
            " |\n",
            " |      >>> for window in dataset:\n",
            " |      ...   print([a.item() for a in window.as_numpy_iterator()])\n",
            " |      [0, 1, 2]\n",
            " |      [3, 4, 5]\n",
            " |      [6]\n",
            " |\n",
            " |      #### Shift\n",
            " |\n",
            " |      The `shift` argument determines the number of input elements to shift\n",
            " |      between the start of each window. If windows and elements are both numbered\n",
            " |      starting at 0, the first element in window `k` will be element `k * shift`\n",
            " |      of the input dataset. In particular, the first element of the first window\n",
            " |      will always be the first element of the input dataset.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\n",
            " |      ...                                           drop_remainder=True)\n",
            " |      >>> for window in dataset:\n",
            " |      ...   print([a.item() for a in window.as_numpy_iterator()])\n",
            " |      [0, 1, 2]\n",
            " |      [1, 2, 3]\n",
            " |      [2, 3, 4]\n",
            " |      [3, 4, 5]\n",
            " |      [4, 5, 6]\n",
            " |\n",
            " |      #### Stride\n",
            " |\n",
            " |      The `stride` argument determines the stride between input elements within a\n",
            " |      window.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2,\n",
            " |      ...                                           drop_remainder=True)\n",
            " |      >>> for window in dataset:\n",
            " |      ...   print([a.item() for a in window.as_numpy_iterator()])\n",
            " |      [0, 2, 4]\n",
            " |      [1, 3, 5]\n",
            " |      [2, 4, 6]\n",
            " |\n",
            " |      #### Nested elements\n",
            " |\n",
            " |      When the `window` transformation is applied to a dataset whos elements are\n",
            " |      nested structures, it produces a dataset where the elements have the same\n",
            " |      nested structure but each leaf is replaced by a window. In other words,\n",
            " |      the nesting is applied outside of the windows as opposed inside of them.\n",
            " |\n",
            " |      The type signature is:\n",
            " |\n",
            " |      ```\n",
            " |      def window(\n",
            " |          self: Dataset[Nest[T]], ...\n",
            " |      ) -> Dataset[Nest[Dataset[T]]]\n",
            " |      ```\n",
            " |\n",
            " |      Applying `window` to a `Dataset` of tuples gives a tuple of windows:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\n",
            " |      ...                                               [6, 7, 8, 9, 10]))\n",
            " |      >>> dataset = dataset.window(2)\n",
            " |      >>> windows = next(iter(dataset))\n",
            " |      >>> windows\n",
            " |      (<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>,\n",
            " |       <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>)\n",
            " |\n",
            " |      >>> def to_numpy(ds):\n",
            " |      ...   return [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      >>>\n",
            " |      >>> for windows in dataset:\n",
            " |      ...   print(to_numpy(windows[0]), to_numpy(windows[1]))\n",
            " |      [1, 2] [6, 7]\n",
            " |      [3, 4] [8, 9]\n",
            " |      [5] [10]\n",
            " |\n",
            " |      Applying `window` to a `Dataset` of dictionaries gives a dictionary of\n",
            " |      `Datasets`:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\n",
            " |      ...                                               'b': [4, 5, 6],\n",
            " |      ...                                               'c': [7, 8, 9]})\n",
            " |      >>> dataset = dataset.window(2)\n",
            " |      >>> def to_numpy(ds):\n",
            " |      ...   return [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      >>>\n",
            " |      >>> for windows in dataset:\n",
            " |      ...   print(tf.nest.map_structure(to_numpy, windows))\n",
            " |      {'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\n",
            " |      {'a': [3], 'b': [6], 'c': [9]}\n",
            " |\n",
            " |      #### Flatten a dataset of windows\n",
            " |\n",
            " |      The `Dataset.flat_map` and `Dataset.interleave` methods can be used to\n",
            " |      flatten a dataset of windows into a single dataset.\n",
            " |\n",
            " |      The argument to `flat_map` is a function that takes an element from the\n",
            " |      dataset and returns a `Dataset`. `flat_map` chains together the resulting\n",
            " |      datasets sequentially.\n",
            " |\n",
            " |      For example, to turn each window into a dense tensor:\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\n",
            " |      ...                                           drop_remainder=True)\n",
            " |      >>> batched = dataset.flat_map(lambda x:x.batch(3))\n",
            " |      >>> for batch in batched:\n",
            " |      ...   print(batch.numpy())\n",
            " |      [0 1 2]\n",
            " |      [1 2 3]\n",
            " |      [2 3 4]\n",
            " |      [3 4 5]\n",
            " |      [4 5 6]\n",
            " |\n",
            " |      Args:\n",
            " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
            " |          of the input dataset to combine into a window. Must be positive.\n",
            " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
            " |          number of input elements by which the window moves in each iteration.\n",
            " |          Defaults to `size`. Must be positive.\n",
            " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
            " |          stride of the input elements in the sliding window. Must be positive.\n",
            " |          The default value of 1 means \"retain every input element\".\n",
            " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
            " |          whether the last windows should be dropped if their size is smaller than\n",
            " |          `size`.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  with_options(self, options, name=None) -> 'DatasetV2'\n",
            " |      Returns a new `tf.data.Dataset` with the given options set.\n",
            " |\n",
            " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
            " |      If options are set multiple times, they are merged as long as different\n",
            " |      options do not use different non-default values.\n",
            " |\n",
            " |      >>> ds = tf.data.Dataset.range(5)\n",
            " |      >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n",
            " |      ...                    cycle_length=3,\n",
            " |      ...                    num_parallel_calls=3)\n",
            " |      >>> options = tf.data.Options()\n",
            " |      >>> # This will make the interleave order non-deterministic.\n",
            " |      >>> options.deterministic = False\n",
            " |      >>> ds = ds.with_options(options)\n",
            " |\n",
            " |      Args:\n",
            " |        options: A `tf.data.Options` that identifies the options the use.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError: when an option is set more than once to a non-default value\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from tensorflow.python.data.ops.dataset_ops.DatasetV2:\n",
            " |\n",
            " |  choose_from_datasets(datasets, choice_dataset, stop_on_empty_dataset=True) -> 'DatasetV2'\n",
            " |      Creates a dataset that deterministically chooses elements from `datasets`.\n",
            " |\n",
            " |      For example, given the following datasets:\n",
            " |\n",
            " |      ```python\n",
            " |      datasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n",
            " |                  tf.data.Dataset.from_tensors(\"bar\").repeat(),\n",
            " |                  tf.data.Dataset.from_tensors(\"baz\").repeat()]\n",
            " |\n",
            " |      # Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\n",
            " |      choice_dataset = tf.data.Dataset.range(3).repeat(3)\n",
            " |\n",
            " |      result = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n",
            " |      ```\n",
            " |\n",
            " |      The elements of `result` will be:\n",
            " |\n",
            " |      ```\n",
            " |      \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        datasets: A non-empty list of `tf.data.Dataset` objects with compatible\n",
            " |          structure.\n",
            " |        choice_dataset: A `tf.data.Dataset` of scalar `tf.int64` tensors between\n",
            " |          `0` and `len(datasets) - 1`.\n",
            " |        stop_on_empty_dataset: If `True`, selection stops if it encounters an\n",
            " |          empty dataset. If `False`, it skips empty datasets. It is recommended to\n",
            " |          set it to `True`. Otherwise, the selected elements start off as the user\n",
            " |          intends, but may change as input datasets become empty. This can be\n",
            " |          difficult to detect since the dataset starts off looking correct.\n",
            " |          Defaults to `True`.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |      Raises:\n",
            " |        TypeError: If `datasets` or `choice_dataset` has the wrong type.\n",
            " |        ValueError: If `datasets` is empty.\n",
            " |\n",
            " |  counter(start=0, step=1, dtype=tf.int64, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` that counts from `start` in steps of size `step`.\n",
            " |\n",
            " |      Unlike `tf.data.Dataset.range`, which stops at some ending number,\n",
            " |      `tf.data.Dataset.counter` produces elements indefinitely.\n",
            " |\n",
            " |      >>> dataset = tf.data.experimental.Counter().take(5)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |      >>> dataset.element_spec\n",
            " |      TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
            " |      >>> dataset = tf.data.experimental.Counter(dtype=tf.int32)\n",
            " |      >>> dataset.element_spec\n",
            " |      TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
            " |      >>> dataset = tf.data.experimental.Counter(start=2).take(5)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [2, 3, 4, 5, 6]\n",
            " |      >>> dataset = tf.data.experimental.Counter(start=2, step=5).take(5)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [2, 7, 12, 17, 22]\n",
            " |      >>> dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [10, 9, 8, 7, 6]\n",
            " |\n",
            " |      Args:\n",
            " |        start: (Optional.) The starting value for the counter. Defaults to 0.\n",
            " |        step: (Optional.) The step size for the counter. Defaults to 1.\n",
            " |        dtype: (Optional.) The data type for counter elements. Defaults to\n",
            " |          `tf.int64`.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A `Dataset` of scalar `dtype` elements.\n",
            " |\n",
            " |  from_generator(generator, output_types=None, output_shapes=None, args=None, output_signature=None, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
            " |\n",
            " |      Deprecated: SOME ARGUMENTS ARE DEPRECATED: `(output_shapes, output_types)`. They will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Use output_signature instead\n",
            " |\n",
            " |      Note: The current implementation of `Dataset.from_generator()` uses\n",
            " |      `tf.numpy_function` and inherits the same constraints. In particular, it\n",
            " |      requires the dataset and iterator related operations to be placed\n",
            " |      on a device in the same process as the Python program that called\n",
            " |      `Dataset.from_generator()`. In particular, using `from_generator` will\n",
            " |      preclude the use of tf.data service for scaling out dataset processing.\n",
            " |      The body of `generator` will not be serialized in a `GraphDef`, and you\n",
            " |      should not use this method if you need to serialize your model and restore\n",
            " |      it in a different environment.\n",
            " |\n",
            " |      The `generator` argument must be a callable object that returns\n",
            " |      an object that supports the `iter()` protocol (e.g. a generator function).\n",
            " |\n",
            " |      The elements generated by `generator` must be compatible with either the\n",
            " |      given `output_signature` argument or with the given `output_types` and\n",
            " |      (optionally) `output_shapes` arguments, whichever was specified.\n",
            " |\n",
            " |      The recommended way to call `from_generator` is to use the\n",
            " |      `output_signature` argument. In this case the output will be assumed to\n",
            " |      consist of objects with the classes, shapes and types defined by\n",
            " |      `tf.TypeSpec` objects from `output_signature` argument:\n",
            " |\n",
            " |      >>> def gen():\n",
            " |      ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
            " |      ...   yield 42, ragged_tensor\n",
            " |      >>>\n",
            " |      >>> dataset = tf.data.Dataset.from_generator(\n",
            " |      ...      gen,\n",
            " |      ...      output_signature=(\n",
            " |      ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n",
            " |      ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
            " |      >>>\n",
            " |      >>> list(dataset.take(1))\n",
            " |      [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
            " |      <tf.RaggedTensor [[1, 2], [3]]>)]\n",
            " |\n",
            " |      There is also a deprecated way to call `from_generator` by either with\n",
            " |      `output_types` argument alone or together with `output_shapes` argument.\n",
            " |      In this case the output of the function will be assumed to consist of\n",
            " |      `tf.Tensor` objects with the types defined by `output_types` and with the\n",
            " |      shapes which are either unknown or defined by `output_shapes`.\n",
            " |\n",
            " |      Note: If `generator` depends on mutable global variables or other external\n",
            " |      state, be aware that the runtime may invoke `generator` multiple times\n",
            " |      (in order to support repeating the `Dataset`) and at any time\n",
            " |      between the call to `Dataset.from_generator()` and the production of the\n",
            " |      first element from the generator. Mutating global variables or external\n",
            " |      state can cause undefined behavior, and we recommend that you explicitly\n",
            " |      cache any external state in `generator` before calling\n",
            " |      `Dataset.from_generator()`.\n",
            " |\n",
            " |      Note: While the `output_signature` parameter makes it possible to yield\n",
            " |      `Dataset` elements, the scope of `Dataset.from_generator()` should be\n",
            " |      limited to logic that cannot be expressed through tf.data operations. Using\n",
            " |      tf.data operations within the generator function is an anti-pattern and may\n",
            " |      result in incremental memory growth.\n",
            " |\n",
            " |      Args:\n",
            " |        generator: A callable object that returns an object that supports the\n",
            " |          `iter()` protocol. If `args` is not specified, `generator` must take no\n",
            " |          arguments; otherwise it must take as many arguments as there are values\n",
            " |          in `args`.\n",
            " |        output_types: (Optional.) A (nested) structure of `tf.DType` objects\n",
            " |          corresponding to each component of an element yielded by `generator`.\n",
            " |        output_shapes: (Optional.) A (nested) structure of `tf.TensorShape`\n",
            " |          objects corresponding to each component of an element yielded by\n",
            " |          `generator`.\n",
            " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
            " |          and passed to `generator` as NumPy-array arguments.\n",
            " |        output_signature: (Optional.) A (nested) structure of `tf.TypeSpec`\n",
            " |          objects corresponding to each component of an element yielded by\n",
            " |          `generator`.\n",
            " |        name: (Optional.) A name for the tf.data operations used by\n",
            " |          `from_generator`.\n",
            " |\n",
            " |      Returns:\n",
            " |        Dataset: A `Dataset`.\n",
            " |\n",
            " |  from_tensor_slices(tensors, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
            " |\n",
            " |      The given tensors are sliced along their first dimension. This operation\n",
            " |      preserves the structure of the input tensors, removing the first dimension\n",
            " |      of each tensor and using it as the dataset dimension. All input tensors\n",
            " |      must have the same size in their first dimensions.\n",
            " |\n",
            " |      >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
            " |      >>> [a.item() for a in dataset.as_numpy_iterator()]\n",
            " |      [1, 2, 3]\n",
            " |\n",
            " |      >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
            " |\n",
            " |      >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
            " |      >>> # scalar tensors.\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
            " |      >>> [(n0.item(), n1.item(), n2.item()) for n0, n1, n2 in\n",
            " |      ...        dataset.as_numpy_iterator()]\n",
            " |      [(1, 3, 5), (2, 4, 6)]\n",
            " |\n",
            " |      >>> # Dictionary structure is also preserved.\n",
            " |      >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
            " |      >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
            " |      ...                                       {'a': 2, 'b': 4}]\n",
            " |      True\n",
            " |\n",
            " |      >>> # Two tensors can be combined into one Dataset object.\n",
            " |      >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
            " |      >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
            " |      >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
            " |      >>> # Both the features and the labels tensors can be converted\n",
            " |      >>> # to a Dataset object separately and combined after.\n",
            " |      >>> features_dataset = Dataset.from_tensor_slices(features)\n",
            " |      >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
            " |      >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
            " |      >>> # A batched feature and label set can be converted to a Dataset\n",
            " |      >>> # in similar fashion.\n",
            " |      >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
            " |      ...                                 [[2, 1], [1, 2]],\n",
            " |      ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
            " |      >>> batched_labels = tf.constant([['A', 'A'],\n",
            " |      ...                               ['B', 'B'],\n",
            " |      ...                               ['A', 'B']], shape=(3, 2, 1))\n",
            " |      >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
            " |      >>> for element in dataset.as_numpy_iterator():\n",
            " |      ...   print(element)\n",
            " |      (array([[1, 3],\n",
            " |             [2, 3]], dtype=int32), array([[b'A'],\n",
            " |             [b'A']], dtype=object))\n",
            " |      (array([[2, 1],\n",
            " |             [1, 2]], dtype=int32), array([[b'B'],\n",
            " |             [b'B']], dtype=object))\n",
            " |      (array([[3, 3],\n",
            " |             [3, 2]], dtype=int32), array([[b'A'],\n",
            " |             [b'B']], dtype=object))\n",
            " |\n",
            " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
            " |      enabled, the values will be embedded in the graph as one or more\n",
            " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
            " |      memory and run into byte limits of graph serialization. If `tensors`\n",
            " |      contains one or more large NumPy arrays, consider the alternative described\n",
            " |      in [this guide](\n",
            " |      https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
            " |\n",
            " |      Args:\n",
            " |        tensors: A dataset element, whose components have the same first\n",
            " |          dimension. Supported values are documented\n",
            " |          [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        Dataset: A `Dataset`.\n",
            " |\n",
            " |  from_tensors(tensors, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
            " |\n",
            " |      `from_tensors` produces a dataset containing only a single element. To slice\n",
            " |      the input tensor into multiple elements, use `from_tensor_slices` instead.\n",
            " |\n",
            " |      >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [array([1, 2, 3], dtype=int32)]\n",
            " |      >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [(array([1, 2, 3], dtype=int32), b'A')]\n",
            " |\n",
            " |      >>> # You can use `from_tensors` to produce a dataset which repeats\n",
            " |      >>> # the same example many times.\n",
            " |      >>> example = tf.constant([1,2,3])\n",
            " |      >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
            " |      >>> list(dataset.as_numpy_iterator())\n",
            " |      [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n",
            " |\n",
            " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
            " |      enabled, the values will be embedded in the graph as one or more\n",
            " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
            " |      memory and run into byte limits of graph serialization. If `tensors`\n",
            " |      contains one or more large NumPy arrays, consider the alternative described\n",
            " |      in [this\n",
            " |      guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
            " |\n",
            " |      Args:\n",
            " |        tensors: A dataset \"element\". Supported values are documented\n",
            " |          [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        Dataset: A `Dataset`.\n",
            " |\n",
            " |  list_files(file_pattern, shuffle=None, seed=None, name=None) -> 'DatasetV2'\n",
            " |      A dataset of all files matching one or more glob patterns.\n",
            " |\n",
            " |      The `file_pattern` argument should be a small number of glob patterns.\n",
            " |      If your filenames have already been globbed, use\n",
            " |      `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
            " |      filename with `list_files` may result in poor performance with remote\n",
            " |      storage systems.\n",
            " |\n",
            " |      Note: The default behavior of this method is to return filenames in\n",
            " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
            " |      to get results in a deterministic order.\n",
            " |\n",
            " |      Example:\n",
            " |        If we had the following files on our filesystem:\n",
            " |\n",
            " |          - /path/to/dir/a.txt\n",
            " |          - /path/to/dir/b.py\n",
            " |          - /path/to/dir/c.py\n",
            " |\n",
            " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
            " |        would produce:\n",
            " |\n",
            " |          - /path/to/dir/b.py\n",
            " |          - /path/to/dir/c.py\n",
            " |\n",
            " |      Args:\n",
            " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
            " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
            " |          pattern(s) that will be matched.\n",
            " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
            " |          Defaults to `True`.\n",
            " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
            " |          seed that will be used to create the distribution. See\n",
            " |          `tf.random.set_seed` for behavior.\n",
            " |        name: Optional. A name for the tf.data operations used by `list_files`.\n",
            " |\n",
            " |      Returns:\n",
            " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
            " |\n",
            " |  load(path, element_spec=None, compression=None, reader_func=None, wait=False) -> 'DatasetV2'\n",
            " |      Loads a previously saved dataset.\n",
            " |\n",
            " |      Example usage:\n",
            " |\n",
            " |      >>> import tempfile\n",
            " |      >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n",
            " |      >>> # Save a dataset\n",
            " |      >>> dataset = tf.data.Dataset.range(2)\n",
            " |      >>> tf.data.Dataset.save(dataset, path)\n",
            " |      >>> new_dataset = tf.data.Dataset.load(path)\n",
            " |      >>> for elem in new_dataset:\n",
            " |      ...   print(elem)\n",
            " |      tf.Tensor(0, shape=(), dtype=int64)\n",
            " |      tf.Tensor(1, shape=(), dtype=int64)\n",
            " |\n",
            " |\n",
            " |      If the default option of sharding the saved dataset was used, the element\n",
            " |      order of the saved dataset will be preserved when loading it.\n",
            " |\n",
            " |      The `reader_func` argument can be used to specify a custom order in which\n",
            " |      elements should be loaded from the individual shards. The `reader_func` is\n",
            " |      expected to take a single argument -- a dataset of datasets, each containing\n",
            " |      elements of one of the shards -- and return a dataset of elements. For\n",
            " |      example, the order of shards can be shuffled when loading them as follows:\n",
            " |\n",
            " |      ```python\n",
            " |      def custom_reader_func(datasets):\n",
            " |        datasets = datasets.shuffle(NUM_SHARDS)\n",
            " |        return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n",
            " |\n",
            " |      dataset = tf.data.Dataset.load(\n",
            " |          path=\"/path/to/data\", ..., reader_func=custom_reader_func)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        path: Required. A path pointing to a previously saved dataset.\n",
            " |        element_spec: Optional. A nested structure of `tf.TypeSpec` objects\n",
            " |          matching the structure of an element of the saved dataset and specifying\n",
            " |          the type of individual element components. If not provided, the nested\n",
            " |          structure of `tf.TypeSpec` saved with the saved dataset is used. Note\n",
            " |          that this argument is required in graph mode.\n",
            " |        compression: Optional. The algorithm to use to decompress the data when\n",
            " |          reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\n",
            " |        reader_func: Optional. A function to control how to read data from shards.\n",
            " |          If present, the function will be traced and executed as graph\n",
            " |          computation.\n",
            " |        wait: If `True`, for snapshots written with `distributed_save`, it reads\n",
            " |          the snapshot while it is being written. For snapshots written with\n",
            " |          regular `save`, it waits for the snapshot until it's finished. The\n",
            " |          default is `False` for backward compatibility. Users of\n",
            " |          `distributed_save` are recommended to set it to `True`.\n",
            " |\n",
            " |      Returns:\n",
            " |        A `tf.data.Dataset` instance.\n",
            " |\n",
            " |      Raises:\n",
            " |        FileNotFoundError: If `element_spec` is not specified and the saved nested\n",
            " |          structure of `tf.TypeSpec` can not be located with the saved dataset.\n",
            " |        ValueError: If `element_spec` is not specified and the method is executed\n",
            " |          in graph mode.\n",
            " |\n",
            " |  random(seed=None, rerandomize_each_iteration=None, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` of pseudorandom values.\n",
            " |\n",
            " |      The dataset generates a sequence of uniformly distributed integer values.\n",
            " |\n",
            " |      `rerandomize_each_iteration` controls whether the sequence of random number\n",
            " |      generated should be re-randomized for each epoch. The default value is False\n",
            " |      where the dataset generates the same sequence of random numbers for each\n",
            " |      epoch.\n",
            " |\n",
            " |      >>> ds1 = tf.data.Dataset.random(seed=4).take(10)\n",
            " |      >>> ds2 = tf.data.Dataset.random(seed=4).take(10)\n",
            " |      >>> print(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\n",
            " |      True\n",
            " |\n",
            " |      >>> ds3 = tf.data.Dataset.random(seed=4).take(10)\n",
            " |      >>> ds3_first_epoch = list(ds3.as_numpy_iterator())\n",
            " |      >>> ds3_second_epoch = list(ds3.as_numpy_iterator())\n",
            " |      >>> print(ds3_first_epoch == ds3_second_epoch)\n",
            " |      True\n",
            " |\n",
            " |      >>> ds4 = tf.data.Dataset.random(\n",
            " |      ...     seed=4, rerandomize_each_iteration=True).take(10)\n",
            " |      >>> ds4_first_epoch = list(ds4.as_numpy_iterator())\n",
            " |      >>> ds4_second_epoch = list(ds4.as_numpy_iterator())\n",
            " |      >>> print(ds4_first_epoch == ds4_second_epoch)\n",
            " |      False\n",
            " |\n",
            " |      Args:\n",
            " |        seed: (Optional) If specified, the dataset produces a deterministic\n",
            " |          sequence of values.\n",
            " |        rerandomize_each_iteration: (Optional) If set to False, the dataset\n",
            " |        generates the same sequence of random numbers for each epoch. If set to\n",
            " |        True, it generates a different deterministic sequence of random numbers\n",
            " |        for each epoch. It is defaulted to False if left unspecified.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        Dataset: A `Dataset`.\n",
            " |\n",
            " |  range(*args, **kwargs) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` of a step-separated range of values.\n",
            " |\n",
            " |      >>> ds = Dataset.range(5)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |      >>> ds = Dataset.range(2, 5)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [2, 3, 4]\n",
            " |      >>> ds = Dataset.range(1, 5, 2)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [1, 3]\n",
            " |      >>> ds = Dataset.range(1, 5, -2)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      []\n",
            " |      >>> ds = Dataset.range(5, 1)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      []\n",
            " |      >>> ds = Dataset.range(5, 1, -2)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [5, 3]\n",
            " |      >>> ds = Dataset.range(2, 5, output_type=tf.int32)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [2, 3, 4]\n",
            " |      >>> ds = Dataset.range(1, 5, 2, output_type=tf.float32)\n",
            " |      >>> [a.item() for a in ds.as_numpy_iterator()]\n",
            " |      [1.0, 3.0]\n",
            " |\n",
            " |      Args:\n",
            " |        *args: follows the same semantics as python's range.\n",
            " |          len(args) == 1 -> start = 0, stop = args[0], step = 1.\n",
            " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1.\n",
            " |          len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\n",
            " |        **kwargs:\n",
            " |          - output_type: Its expected dtype. (Optional, default: `tf.int64`).\n",
            " |          - name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        Dataset: A `RangeDataset`.\n",
            " |\n",
            " |      Raises:\n",
            " |        ValueError: if len(args) == 0.\n",
            " |\n",
            " |  sample_from_datasets(datasets, weights=None, seed=None, stop_on_empty_dataset=False, rerandomize_each_iteration=None) -> 'DatasetV2'\n",
            " |      Samples elements at random from the datasets in `datasets`.\n",
            " |\n",
            " |      Creates a dataset by interleaving elements of `datasets` with `weight[i]`\n",
            " |      probability of picking an element from dataset `i`. Sampling is done without\n",
            " |      replacement. For example, suppose we have 2 datasets:\n",
            " |\n",
            " |      ```python\n",
            " |      dataset1 = tf.data.Dataset.range(0, 3)\n",
            " |      dataset2 = tf.data.Dataset.range(100, 103)\n",
            " |      ```\n",
            " |\n",
            " |      Suppose that we sample from these 2 datasets with the following weights:\n",
            " |\n",
            " |      ```python\n",
            " |      sample_dataset = tf.data.Dataset.sample_from_datasets(\n",
            " |          [dataset1, dataset2], weights=[0.5, 0.5])\n",
            " |      ```\n",
            " |\n",
            " |      One possible outcome of elements in sample_dataset is:\n",
            " |\n",
            " |      ```\n",
            " |      print(list(sample_dataset.as_numpy_iterator()))\n",
            " |      # [100, 0, 1, 101, 2, 102]\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |        datasets: A non-empty list of `tf.data.Dataset` objects with compatible\n",
            " |          structure.\n",
            " |        weights: (Optional.) A list or Tensor of `len(datasets)` floating-point\n",
            " |          values where `weights[i]` represents the probability to sample from\n",
            " |          `datasets[i]`, or a `tf.data.Dataset` object where each element is such\n",
            " |          a list. Defaults to a uniform distribution across `datasets`.\n",
            " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
            " |          seed that will be used to create the distribution. See\n",
            " |          `tf.random.set_seed` for behavior.\n",
            " |        stop_on_empty_dataset: If `True`, sampling stops if it encounters an empty\n",
            " |          dataset. If `False`, it continues sampling and skips any empty datasets.\n",
            " |          It is recommended to set it to `True`. Otherwise, the distribution of\n",
            " |          samples starts off as the user intends, but may change as input datasets\n",
            " |          become empty. This can be difficult to detect since the dataset starts\n",
            " |          off looking correct. Default to `False` for backward compatibility.\n",
            " |        rerandomize_each_iteration: An optional `bool`. The boolean argument\n",
            " |        controls whether the sequence of random numbers used to determine which\n",
            " |        dataset to sample from will be rerandomized each epoch. That is, it\n",
            " |        determinies whether datasets will be sampled in the same order across\n",
            " |        different epochs (the default behavior) or not.\n",
            " |\n",
            " |      Returns:\n",
            " |        A dataset that interleaves elements from `datasets` at random, according\n",
            " |        to `weights` if provided, otherwise with uniform probability.\n",
            " |\n",
            " |      Raises:\n",
            " |        TypeError: If the `datasets` or `weights` arguments have the wrong type.\n",
            " |        ValueError:\n",
            " |          - If `datasets` is empty, or\n",
            " |          - If `weights` is specified and does not match the length of `datasets`.\n",
            " |\n",
            " |  zip(*args, datasets=None, name=None) -> 'DatasetV2'\n",
            " |      Creates a `Dataset` by zipping together the given datasets.\n",
            " |\n",
            " |      This method has similar semantics to the built-in `zip()` function\n",
            " |      in Python, with the main difference being that the `datasets`\n",
            " |      argument can be a (nested) structure of `Dataset` objects. The supported\n",
            " |      nesting mechanisms are documented\n",
            " |      [here] (https://www.tensorflow.org/guide/data#dataset_structure).\n",
            " |\n",
            " |      >>> # The datasets or nested structure of datasets `*args` argument\n",
            " |      >>> # determines the structure of elements in the resulting dataset.\n",
            " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
            " |      >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
            " |      >>> ds = tf.data.Dataset.zip(a, b)\n",
            " |      >>> [(i.item(), j.item()) for i, j in ds.as_numpy_iterator()]\n",
            " |      [(1, 4), (2, 5), (3, 6)]\n",
            " |      >>> ds = tf.data.Dataset.zip(b, a)\n",
            " |      >>> [(i.item(), j.item()) for i, j in ds.as_numpy_iterator()]\n",
            " |      [(4, 1), (5, 2), (6, 3)]\n",
            " |      >>>\n",
            " |      >>> # The `datasets` argument may contain an arbitrary number of datasets.\n",
            " |      >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
            " |      ...                                            #       [9, 10],\n",
            " |      ...                                            #       [11, 12] ]\n",
            " |      >>> ds = tf.data.Dataset.zip(a, b, c)\n",
            " |      >>> for i, j, k in ds.as_numpy_iterator():\n",
            " |      ...   print(i.item(), j.item(), k)\n",
            " |      1 4 [7 8]\n",
            " |      2 5 [ 9 10]\n",
            " |      3 6 [11 12]\n",
            " |      >>>\n",
            " |      >>> # The number of elements in the resulting dataset is the same as\n",
            " |      >>> # the size of the smallest dataset in `datasets`.\n",
            " |      >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
            " |      >>> ds = tf.data.Dataset.zip(a, d)\n",
            " |      >>> [(i.item(), j.item()) for i, j in ds.as_numpy_iterator()]\n",
            " |      [(1, 13), (2, 14)]\n",
            " |\n",
            " |      Args:\n",
            " |        *args: Datasets or nested structures of datasets to zip together. This\n",
            " |          can't be set if `datasets` is set.\n",
            " |        datasets: A (nested) structure of datasets. This can't be set if `*args`\n",
            " |          is set. Note that this exists only for backwards compatibility and it is\n",
            " |          preferred to use *args.\n",
            " |        name: (Optional.) A name for the tf.data operation.\n",
            " |\n",
            " |      Returns:\n",
            " |        A new `Dataset` with the transformation applied as described above.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.data.ops.dataset_ops.DatasetV2:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from collections.abc.Iterable:\n",
            " |\n",
            " |  __class_getitem__ = GenericAlias(...)\n",
            " |      Represent a PEP 585 generic type\n",
            " |\n",
            " |      E.g. for t = list[int], t.__origin__ is list and t.__args__ is (int,).\n",
            " |\n",
            " |  __subclasshook__(C)\n",
            " |      Abstract classes can override this to customize issubclass().\n",
            " |\n",
            " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
            " |      It should return True, False or NotImplemented.  If it returns\n",
            " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
            " |      overrides the normal algorithm (and the outcome is cached).\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.framework.composite_tensor.CompositeTensor:\n",
            " |\n",
            " |  __tf_tracing_type__(self, context)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(39, activation='linear')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WNlJtuPMpD7",
        "outputId": "1e516331-616d-4d5d-d1c5-f3ca3309cfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "p7WInYEPOmG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(image_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inuKZw-ZO9lq",
        "outputId": "24c550da-3012-49bd-8eb4-cf25f675156f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/nn.py:717: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26064/26064\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1058s\u001b[0m 41ms/step - accuracy: 0.8661 - loss: 0.6071\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a7ad9dc1700>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation_path = './data/Validation'\n",
        "\n",
        "image_size = (32, 32) # Define your desired image size\n",
        "batch_size = 32 # Define your desired batch size\n",
        "\n",
        "image_validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_validation_path,\n",
        "    labels='inferred', # Infer labels from subdirectory names\n",
        "    label_mode='int',   # Labels as integers\n",
        "    image_size=image_size,\n",
        "    interpolation='nearest',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pODg3dbPQBn",
        "outputId": "8982dfb9-7bea-4323-8dd0-9a46cbc8f4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22524 files belonging to 39 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((image_validation_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2YLZVRQT0pz",
        "outputId": "f743ea12-d2b7-4c19-b53d-a2c5e29ed3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one batch\n",
        "for images, labels in image_validation_dataset.take(1):\n",
        "    print(\"Batch shape:\", images.shape)   # e.g. (32, 32, 32, 1)\n",
        "    print(\"Labels shape:\", labels.shape)  # e.g. (32,)\n",
        "\n",
        "    # First image and label in the batch\n",
        "    first_image = images[0].numpy()\n",
        "    first_label = labels[0].numpy()\n",
        "\n",
        "    print(\"First image shape:\", first_image.shape)\n",
        "    print(\"First label:\", first_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDa8IRlnT_vs",
        "outputId": "12f581d2-78cb-4c1e-af21-de2415460037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: (32, 32, 32, 3)\n",
            "Labels shape: (32,)\n",
            "First image shape: (32, 32, 3)\n",
            "First label: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(first_image.squeeze(), cmap=\"gray\")  # squeeze if shape is (32,32,1)\n",
        "plt.title(f\"Label: {first_label}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "ERvIOR7-UVLU",
        "outputId": "9a13b246-29a6-4696-f347-38186695de62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(image_validation_dataset))"
      ],
      "metadata": {
        "id": "q0Bz3L_hUxBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KGM60cAYVY7_",
        "outputId": "64ab73dd-a303-4558-bc89-bddc86445554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]], shape=(32, 32, 3), dtype=uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_image = 0\n",
        "first_label = 0\n",
        "for images, labels in image_validation_dataset.take(1):\n",
        "    first_image = images[0]   # shape (32,32,1)\n",
        "    first_label = labels[0]\n",
        "    break\n",
        "first_image_batch = tf.expand_dims(first_image, axis=0)  # shape (1,32,32,1)\n",
        "logits = model.predict(first_image_batch)   # shape (1, num_classes)\n",
        "probs = tf.nn.softmax(logits[0])  # convert to probabilities\n",
        "predicted_class = tf.argmax(probs).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFofzeiuVk9J",
        "outputId": "6a385a06-8398-46c9-a2da-ab420f38b3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"True label:\", first_label.numpy())\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "print(\"Class probabilities:\", probs.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckRFmfUuWJgo",
        "outputId": "3d88d477-4ec6-4d6c-cc05-ae551d8f89fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label: 7\n",
            "Predicted class: 7\n",
            "Class probabilities: [0.02455982 0.02455982 0.02455982 0.02455982 0.02455983 0.02455982\n",
            " 0.02455982 0.06670696 0.02455982 0.02455982 0.02455992 0.02455982\n",
            " 0.02456005 0.02455982 0.02455982 0.02455982 0.02455982 0.02455982\n",
            " 0.02455982 0.02455982 0.02455982 0.02455982 0.02455982 0.02455982\n",
            " 0.02455982 0.02455983 0.02455982 0.02455982 0.02455982 0.02455983\n",
            " 0.02455982 0.02455982 0.02455984 0.02455983 0.02455983 0.02455982\n",
            " 0.02455982 0.02457919 0.02455982]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {}\n",
        "for i in range(len(image_validation_dataset.class_names)):\n",
        "  dictionary[i] = image_validation_dataset.class_names[i]\n",
        "print(\"Final prediction :\",dictionary[predicted_class])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig5n-GtaWpwR",
        "outputId": "34011d97-15d3-4175-89d2-a0ef46682b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final prediction : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(first_image.numpy().squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"Label: {first_label}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "Uh7Kn2IsXGMY",
        "outputId": "7aeec75d-a6bb-4343-814e-30b2047ba035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEwNJREFUeJzt3X2s13XZwPHrnAMcTBkIiJpCxMqSzdJkWonr5FIoW4NKZ7QcG7FWOZ0re1r5sLmcWw8sKXKVE5NmD1prw5Wu0q2NQFZqFCY1kYXIswGS53A45/7j3n3t9oa6v5fxld+R12vzn7PrfPr+fud3zpvvgd9V1/Dw8HAAQER0H+0LAKBziAIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQKvSBs3boyurq74yle+csTOfOihh6KrqyseeuihI3YmdBpRoGPceeed0dXVFWvXrj3al9KK6dOnR1dX12H/e/3rX3+0Lw8iImLU0b4AOFYsWbIk9u3b96KPPf300/HFL34xLrnkkqN0VfBiogAvk3nz5h3ysZtvvjkiIj784Q+/zFcDh+fXR4woAwMDcf3118e5554b48ePj+OPPz4uvPDC+M1vfvMvP+frX/96vOY1r4njjjsu3vGOd8S6desOmXniiSfigx/8YEycODHGjh0bs2bNip///Of/7/Xs378/nnjiidixY8dLejw/+MEP4rWvfW28/e1vf0mfD0eaKDCi7NmzJ7773e9GX19f3HrrrXHjjTfG9u3bY86cOfHoo48eMn/XXXfFN77xjfjkJz8Zn//852PdunVx0UUXxdatW3PmT3/6U7z1rW+N9evXx+c+97n46le/Gscff3zMmzcvfvrTn/7b61mzZk2ceeaZsXTp0vJj+cMf/hDr16+PBQsWlD8X2uLXR4woJ554YmzcuDHGjBmTH1u8eHG88Y1vjNtuuy2+973vvWj+r3/9a2zYsCFOO+20iIiYO3dunH/++XHrrbfG1772tYiIuOaaa2LatGnxyCOPRG9vb0REfOITn4jZs2fHZz/72Zg/f34rj2XFihUR4VdHdBZ3CowoPT09GYShoaHYtWtXDA4OxqxZs+L3v//9IfPz5s3LIEREnHfeeXH++efH/fffHxERu3btil//+tdx+eWXx969e2PHjh2xY8eO2LlzZ8yZMyc2bNgQmzdv/pfX09fXF8PDw3HjjTeWHsfQ0FDcc889cc4558SZZ55Z+lxokygw4ixfvjze9KY3xdixY2PSpElx0kknxcqVK+Mf//jHIbOH+6eeZ5xxRmzcuDEi/vtOYnh4OL70pS/FSSed9KL/brjhhoiI2LZt2xF/DA8//HBs3rzZXQIdx6+PGFHuvvvuWLhwYcybNy+uu+66mDJlSvT09MQtt9wSf/vb38rnDQ0NRUTEpz/96ZgzZ85hZ173utf9R9d8OCtWrIju7u740Ic+dMTPhv+EKDCi/OQnP4kZM2bEfffdF11dXfnx//lT/f+1YcOGQz725JNPxvTp0yMiYsaMGRERMXr06HjXu9515C/4MPr7++Pee++Nvr6+ePWrX/2y/G9CU359xIjS09MTERHDw8P5sdWrV8eqVasOO/+zn/3sRX8nsGbNmli9enW8+93vjoiIKVOmRF9fX9x+++2xZcuWQz5/+/bt//Z6Xso/Sb3//vvjueee86sjOpI7BTrOHXfcEb/4xS8O+fg111wT733ve+O+++6L+fPnx6WXXhpPPfVUfPvb346ZM2ce8m7hiP/+1c/s2bPj4x//ePT398eSJUti0qRJ8ZnPfCZnvvnNb8bs2bPjrLPOisWLF8eMGTNi69atsWrVqvj73/8ejz322L+81jVr1sQ73/nOuOGGGxr/ZfOKFSuit7c3PvCBDzSah5eTKNBxli1bdtiPL1y4MBYuXBjPPvts3H777fHLX/4yZs6cGXfffXf8+Mc/PuyiuiuvvDK6u7tjyZIlsW3btjjvvPNi6dKlceqpp+bMzJkzY+3atXHTTTfFnXfeGTt37owpU6bEOeecE9dff/0RfWx79uyJlStXxqWXXhrjx48/omfDkdA1/L/vwwE4pvk7BQCSKACQRAGAJAoAJFEAIIkCAKnx+xT+90qBIz3vX8W+/Hx94NjT5HvZnQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQGrt/6O5U/blVHc2VXTKY4yI6O6u9X1oaKilK4FmOuk1W/050Unf+0eaOwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkLqGG75fu811EW3y9vXDa/PrWTm7urqgcvax8rVsU09PT2n+4MGDLV3JsaPN13iTeXcKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBpVFsHd8qOmk7af9Pmc3Ks7Hgaqddd1d3d/M9r1f1RlbNHjx5dOrvyOhwcHCydPWpU7cdV5bXSSTubjvbPTncKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC1tuaCY1ebKxqsuThUdcXJe97znsaz73vf+0pn79y5s/Hs97///dLZf/7zn0vzlddK5fmOqD3n1RUa1e+JI82dAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAam330UjdUVPZaVJ9jJX56j6b6rVUzq/uhanueuFQlf03r3rVq0pnv/nNb248u2DBgtLZlWvp6ekpnf3lL3+5NP/cc881nm1z31Db38tHmjsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBaW3MxUrX5FvPK2/qrb7uvrgwYqasoKo+zkx5j9eszceLExrNXXnll6exFixY1nq2u0Kiorn/o7+9v7fxRo2o/CgcHBxvPHu21FVXuFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0ojcfVTZadLm3pHqvpRx48Y1nr3ssstKZx84cKA0/6tf/arx7KZNm0pnt7mfqDJf3a3T5mvluOOOK81fccUVjWevu+660tknn3xyab5iYGCg8eyuXbtKZ48ZM6Y0X9mVVP3+qb62RhJ3CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjcg1F22uI6i8ff20004rnf3Rj3608eyCBQtKZ1dXAFTWLnzrW98qnV1dXdGW6utk7NixpfnKSofp06eXzr7gggsaz06YMKF09uDgYOPZF154oXT2Pffc03j2rrvuKp29d+/e0nzlddjdXfvz8dDQUGm+4miv8XGnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRuTuo4pRo2oP8cQTT2w8+7GPfax09lVXXdV4trqHZ9++faX5yvPS09NTOrtTdh9VVffITJ06tfHs4sWLS2dffPHFjWerr/Fnnnmm8Wx179Vtt93WeLa6V6n6uqrsEKrMvtK5UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASK/43UcnnHBCaf4jH/lI49lFixaVzh43blzj2YGBgdLZq1evLs1v2LCh8ezQ0FDp7JHqDW94Q2n+5ptvbjx7ySWXlM7u7m7+57X9+/eXzv7hD3/YeHb58uWlsyuv207akdVJ13K0uVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGlErrno7e1tPDt//vzS2ddee23j2SlTppTOrqyLeOCBB0pn33TTTaX5xx57rPHs6NGjS2dXVgZ00nqBU089tTQ/derUxrNtPoerVq0qnV1ZXbFly5bS2ZX1HJXZiPq6leHh4dJ8pzja1+1OAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjcjdR6ecckrj2QsuuKB09umnn169nMa2b9/eePaOO+4onf3kk0+W5nt6ekrzFZUdNdX9N5W9MJMnTy6d/f73v780f/bZZ5fmK/bu3dt49umnny6dvXv37urlNFbdT9Qpurq6Wptv8zmpXncT7hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBqRay5mz57deHbu3LktXknNySef3Hh22bJlpbM3b95cmn/00Ucbz65bt650dn9/f+PZ6gqAqVOnNp698MILS2efe+65pfmK6mqJW265pfHs0qVLS2f/85//LM0fCyrrU17KfFvauA53CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAaUTuPurubt6yrq6uFq+kPZU9SS9l/i1veUtpnv9M5TUbUXvdVvdHdYq2vzc7ZT9RVeV5sfsIgFaJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpI9ZcVN/uvmbNmsazv/3tb0tnX3755aV5aGL8+PGl+auuuqrx7MDAQOns73znO41nn3/++dLZFZ20hqL6M6jNaz/az4s7BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA1DXccNFGdTdIRXd3rU2jRjVf2TR58uTS2W9729saz/b19ZXOnjZtWuPZvXv3ls4eO3ZsaX7ChAmNZ4eGhkpnn3XWWY1nTznllNLZbao+53/84x8bzz744IOlsx944IHGs48//njp7H379pXmeeVo8uPenQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBqRu48qqnt7enp6WpmNiBgYGCjNt2nKlCmNZ8eNG1c6e8GCBY1nP/WpT5XOHj9+fGm+Yu3ataX5a6+9tvHs7373u9LZ1ddtp5xNZ7P7CIASUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjWo6WF1z0XB7RkR01tvuK9dSeYwRteewenbVtm3bGs/u2bOndPbg4GDj2QMHDpTOrnj++edL8/fee29p/pFHHmk82+a6lYMHD5bOhn/HnQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQGq8+6jtXTwVbe4QqsxXzx6p+2xmzZpVmr/iiisaz06ePLl6OY1t3bq1NP/UU0+V5itfo+prpbIrqZN2hzHyuVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKnxmotOUlkZ0N3dXvfaXP1RWYkRUX+cZ599duPZuXPnls4eP35849nqc/j44483nv3CF75QOvvBBx8szVfWXPT29pbO7u/vbzxb/dpbi8G/404BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC1tvuoq6ur8WybO4Q66ezKrpyqCRMmlOYvu+yyxrNXX3116ezRo0c3nt2+fXvp7JUrVzae/ctf/lI6u/r1rMy/8MILpbMr3z92GXEkuVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKm1NRdtrpeo6JTriGh39Ud1jUJl5UZ1jcKBAwcazz777LOlsx9++OHGs5s2bSqdPTg4WJofNar5t0/17E563XJscacAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAaL2+p7O2JsLvlcNp8Ti6++OLS/EUXXdR4trInKaK2E2j06NGlsydNmtR4dsaMGaWzq3uYJk6c2Hh2y5YtpbOru6zgSHGnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBS430Ex8raiso6j+pzMmHChMazixYtKp199dVXl+anTZtWmm/LGWecUZqvPC8bN24snf3MM8+U5nfv3l2a7xRtvsYZ+dwpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkxruPKvtSImo7U0bq2VW9vb2NZ08//fTS2RMnTqxeTkcYHBwsze/fv7/x7L59+0pn7927tzTf09PTePbgwYOls+FocacAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAa7z5qcydQm2e3qbu7vaZOnjy5NH/CCSe0dCV1mzZtajy7bNmy0tnLly9vPLt79+7S2VWdsoOr+jocqd9vvDzcKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA1DXc8D3v1bfpc6gxY8Y0nq2urZg7d25pvq+vr/Hsrl27Smf/6Ec/ajy7fv360tkDAwOl+YqDBw+2dnab2lyhwStLk6+9OwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNTa7qPu7ua9qe5iaXN3S+W6h4aGWruO3t7e0nx/f39LVwLNVX5O2MH08rP7CIASUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgtbbmgkN1ygoNDq/y9YnonDUNnXIddD5rLgAoEQUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkOw+AjhG2H0EQIkoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBGHe0LADpXdb1Nw605dDB3CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqfHuIztNAF753CkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkP4LP/DEud927gAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EP1B4OD8XZxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}